{
  
    
        "post0": {
            "title": "Random Matrix Theory in Molecular Classification",
            "content": "Motivation . I came across with this paper about denoising activity data using random matrix theory (RMT). I never heard about RMT before and it seemed an interesting approach, so I thought I explore the method by trying myself. . Chemicals are often represented in so called chemical fingerprint. The chemical fingerprint is a vector of bit information where each bit represents a presence or an absence of certain chemical groups. We can assume the presence of a set of certain chemical groups is correlated with binding of ligand. Using the RMT framework, we can removes noise in the fingerprints and aim to obtain the set of fingerprints that is significant. . Let&#39;s dive in how it works. . RMT Framework . The RMT approaches this by computing correlation matrix and taking eigenvalue vectors of each column. By taking the eigenvalues from the correlation matrix, the components having insignificant correlation will have small eigenvalues whereas the components having significant correlation will have large eigenvalues. . Let&#39;s suppose there are $N$ fingerprint vector ($ mathbf{f}$) with size $p$. They are arranged in a matrix, $A = [ mathbf{f}_1, mathbf{f}_2, cdots, mathbf{f}_N] in mathbb{R}^{N times p}$. The matrix is normalized by subtract column mean and divide the colunms with standard deviation. The correlation matrix of $N times N$ is constructed as $C = A^T A / N$. . If the entries in the matrix $A$ is i.i.d, then the eigenvalues of $A$ follows the Marcenko–Pastur (MP) distribution. . $$ rho( lambda) = frac{ sqrt{( lambda_+ - lambda) ( lambda - lambda_-)}}{2 pi gamma lambda}$$ . where $ lambda_{ pm} = left({1 pm sqrt gamma} right)^2$ and $ lambda = p / N$. Let&#39;s numerically validate this. . %matplotlib inline import matplotlib.pyplot as plt import matplotlib from io import BytesIO import pandas as pd import numpy as np from IPython.display import SVG # RDKit import rdkit from rdkit.Chem import PandasTools from rdkit import Chem from rdkit.Chem import AllChem from rdkit.Chem import DataStructs from rdkit.Chem import rdMolDescriptors from rdkit.Chem import rdRGroupDecomposition from rdkit.Chem.Draw import IPythonConsole #Needed to show molecules from rdkit.Chem import Draw from rdkit.Chem import rdDepictor from rdkit.Chem.Draw import rdMolDraw2D from rdkit.Chem.Draw.MolDrawing import MolDrawing, DrawingOptions #Only needed if modifying defaults DrawingOptions.bondLineWidth=1.8 IPythonConsole.ipython_useSVG=True from rdkit import RDLogger RDLogger.DisableLog(&#39;rdApp.warning&#39;) print(rdkit.__version__) # misc from tqdm.notebook import tqdm from math import pi, sqrt . . 2020.03.2 . Random Data . N = 500 p = 100 A = np.random.normal(size=(N, p)) . Here&#39;s a matrix $A$ with randomly drawn values. Subtract mean and standard deviation to make the column mean as zero and the variance as 1. According to RMT, the eigen value distribution of the correlation matrix of $A$ will follow the MP distribution. . mean = np.mean(A, axis=0) std = np.std(A, axis=0) A = (A - mean) / std C = np.dot(A.T, A) / A.shape[0] # correlation matrix w, v = np.linalg.eig(C) # w = eigenvalues, v = eigenvectors gamma = A.shape[1] / A.shape[0] gamma_p = (1 + sqrt(gamma))**2 gamma_m = (1 - sqrt(gamma))**2 rho = lambda x: sqrt(np.clip(gamma_p - x, 0, None) * np.clip(x - gamma_m, 0, None)) / (2*pi*gamma*x) x = np.arange(0., np.max(np.real(w)), 0.1) y = np.array([rho(_) for _ in x]) plt.hist(np.real(w), density=True) plt.plot(x, y) plt.xlabel(&#39;Eigenvalue&#39;) plt.ylabel(&#39;Prob.&#39;) plt.show() . . /Users/sunhwan/miniconda3/lib/python3.7/site-packages/ipykernel/__main__.py:12: RuntimeWarning: invalid value encountered in double_scalars . We can see the eigenvalue distribution from a random matrix (blue bar) follows the MP distribution (yellow curve). Now, if the input matrix contains significant correlation, those eigenvalues correspond to the correlation will deviates from the MP distribution. . Let&#39;s take molecular fingerprint data and perform the same analysis. If we select a set of compounds that binds to a particular receptor, and the ligands have similar chemical features, this will be reflected in the molecular fingerprint. The fingerprint vectors will be no longer random, and some eigenvalues will rise above the MP distribution. . Molecular Fingerprint Data . The author used data from ChEMBL database, but I&#39;ll use data from DUD-E database. DUD-E database contains sets of active compounds as well as inactive compounds per target and widely used in a benchmark for drug activity classification. The inactive compounds are not meant to be &quot;true&quot; inactive, but they are synthetically selected to match physico-chemical property yet having different 3D geometry. . There was a recent publication where 3D deep neural network algorithm for scoring/classification actually learned the features in the dataset, so I thought it would be interesting to actually see this effect. . s = Chem.SmilesMolSupplier(&#39;files/dude/ampc/actives_final.ism&#39;) mols_active = [m for m in s] s = Chem.SmilesMolSupplier(&#39;files/dude/ampc/decoys_final.ism&#39;) mols_decoy = [m for m in s] print(len(mols_active), len(mols_decoy)) . 535 35749 . I&#39;ll use the data for target AMPC for this post. There are total 535 active and 35749 decoy compounds in the dataset. Let&#39;s compute the Morgan fingerprint with radius 3 and size 1024 bits. Morgan fingerprint with radius 3 is equivalent to ECFP6 fingerprint. . p = 1024 fps_active = [np.array(AllChem.GetMorganFingerprintAsBitVect(m,3,p)) for m in mols_active] fps_decoy = [np.array(AllChem.GetMorganFingerprintAsBitVect(m,3,p)) for m in mols_decoy] plt.figure(figsize=(12, 8)) plt.imshow(fps_active, interpolation=None, cmap=plt.cm.gray) plt.ylabel(&#39;Ligand&#39;) plt.xlabel(&#39;Fingerprint&#39;) plt.show() . . The plot of fingerprints clearly shows some bits appeare more frequently than the others. Let&#39;s normalize the fingerprint vectors and compute the correlation matrix. Finally take the eigendecomposition of the correlation matrix. . A = np.array(fps_active) # remove columns that are all zeros mean = np.mean(A, axis=0) std = np.std(A, axis=0) column_indices = ~((mean == 0) &amp; (std == 0)) A_reduced = A[:, column_indices] # normalize mean = np.mean(A_reduced, axis=0) std = np.std(A_reduced, axis=0) A_normed = (A_reduced - mean) / std # correlation matrix C = np.dot(A_normed.T, A_normed) / A.shape[0] # correlation matrix w, v = np.linalg.eig(C) # w = eigenvalue, v = eigenvetors . We are ready to plot the eigenvalue distribution and MP distribution. . def MP_distribution(N, p): &quot;&quot;&quot;return MP distribution function based on the shape of input matrix&quot;&quot;&quot; gamma = p / N gamma_p = (1 + sqrt(gamma))**2 gamma_m = (1 - sqrt(gamma))**2 rho = lambda x: sqrt(np.clip(gamma_p - x, 0, None) * np.clip(x - gamma_m, 0, None)) / (2*pi*gamma*x) return rho plt.hist(np.real(w), bins=50, density=True) # plot eigenvalue distribution N, p = A_normed.shape rho = MP_distribution(N, p) # MP distribution w = np.real(w) # real values of eigenvalues x = np.arange(0., np.max(w), 0.1) y = np.array([rho(_) for _ in x]) plt.plot(x, y) # plot MP distribution plt.ylabel(&#39;Probability&#39;) plt.xlabel(&#39;Eigenvalues&#39;) plt.ylim(0, 0.2) plt.show() . . /Users/sunhwan/miniconda3/lib/python3.7/site-packages/ipykernel/__main__.py:8: RuntimeWarning: invalid value encountered in double_scalars . Interesting! There are several eigenvalues deviates from the MP distribution. The RMT suggests the eigenvalues greater than $ left( 1 + sqrt gamma right)^2$ are significant. . def MP_threshold(N, p): &quot;&quot;&quot;return MP threshold based on the shape of input matrix&quot;&quot;&quot; gamma = p / N gamma_p = (1 + sqrt(gamma))**2 return gamma_p th = MP_threshold(N, p) indices = np.argwhere(w &gt; th).flatten() print(len(indices)) . 43 . There are total of 43 eigenvalues above the significance threshold. In other words, the eigenvectors corresponds with these 43 eigenvalues represents the chemical subspace that facilitate the binding of this receptor. . An &quot;ideal&quot; ligands will have fingerprint close to this chemical subspace we just discovered and the ligands lacks these fingerprint features, will lie farther away from this subspace. . Classification of active ligands . Define a subspace consists of $m$ eigenvectors discovered above; $ mathbf{V} = ( mathbf{v}_1, mathbf{v}_2, cdots, mathbf{v}_m)$. For a new ligand with fingerprint vector $ mathbf{u}$, we can compute the projection of this vector onto the subspace $ mathbf{V}$. The projection is defined as . $$ mathbf{u}_p = sum_i^m ( mathbf{v}_i cdot mathbf{u}) mathbf{v}_i$$ . The distance between the $ mathbf{u}$ and the projection vector $ mathbf{u}_p$, defined as $ left Vert mathbf{u} - mathbf{u}_p right Vert$, is the measure of similarity between the new ligands and the ligands known to binds to the receptor. If we compute similarity measure using known active compounds and inactive compounds, then we should be able to separate them. . V = np.real(np.array([v[i] for i in indices])) A_active = np.array(fps_active)[:, column_indices] A_decoy = np.array(fps_decoy)[:, column_indices] # normalize u_active = (A_active - mean) / std u_decoy = (A_decoy - mean) / std # projection u_p_active = np.dot(np.einsum(&#39;ij,kj-&gt;ki&#39;, V, u_active), V) u_p_decoy = np.dot(np.einsum(&#39;ij,kj-&gt;ki&#39;, V, u_decoy), V) # distance dist_active = np.linalg.norm(u_p_active - u_active, axis=1) dist_decoy = np.linalg.norm(u_p_decoy - u_decoy, axis=1) . # active prob_active, bins_active = np.histogram(dist_active, bins=12, density=True) x_active = (bins_active[1:] + bins_active[:-1]) / 2 plt.plot(x_active, prob_active) # decoy prob_decoy, bins_decoy = np.histogram(dist_decoy, bins=25, density=True) x_decoy = (bins_decoy[1:] + bins_decoy[:-1]) / 2 plt.plot(x_decoy, prob_decoy) plt.xlabel(&#39;Distance from V&#39;) plt.ylabel(&#39;Probability&#39;) plt.show() . . Above is the distribution of distance from the subspace $ mathbf{V}$ of known active compounds (blue) and synthetic decoy inactive compounds (orange). They are clearly separated along the distance. Let&#39;s take a look at active compounds that are close to the subspace $ mathbf{V}$, which contains important chemical features, also known as pharmacophores. . indices = np.argsort(dist_active)[:20] # top 20 compounds having smallest distance ms = [mols_active[i] for i in indices] for m in ms: tmp=AllChem.Compute2DCoords(m) img = Draw.MolsToGridImage(ms,molsPerRow=4,subImgSize=(200,200),legends=[x.GetProp(&quot;_Name&quot;) for x in ms]) img . OH OH N O N 48728 OH OH N O N 48709 OH OH N O N 48667 OH OH N N O N N 140318 HO O O 17390 OH O O 219708 OH S O O 203780 OH S O O 204793 OH OH N OH O N OH 107783 OH N O N 54544 HO O O 17383 OH N HO O N OH 53627 OH OH O N O N O 108765 OH N O N 54239 O O NH HO O HN H2N O 157800 OH OH N N O N N 140360 OH N S O O HN O O 258080 O S O O N HO NH O O 231785 HO OH N O N 108644 OH OH N NH N O N NH N 127902 From the above set of compounds, one can attempt to derive scaffold that can be used to design a compounds explore the chemical space around them. . Let&#39;s also take a look at the inactive compounds that have small distance to the important subspace $ mathbf{V}$. . indices = np.argsort(dist_decoy)[:20] # top 20 compounds having smallest distance ms = [mols_decoy[i] for i in indices] for m in ms: tmp=AllChem.Compute2DCoords(m) img = Draw.MolsToGridImage(ms,molsPerRow=4,subImgSize=(200,200),legends=[x.GetProp(&quot;_Name&quot;) for x in ms]) img . HN C32134625 O HN N C08672864 O O- C11628730 N N Cl C01845610 O O Cl O O- O O- Cl C17188164 N N Cl C39370949 O O O C05774943 O O Cl Cl C02885657 N H2+ O O- O O- Cl C04203123 O O O O- Br C11948728 O O C01685700 Cl Cl N H O O N H Cl Cl C12403749 O O O- Cl C41959900 S N H2N S O N H N C19503917 O O- O Cl C26544579 O O- S N N S O O- C16477157 O O- S N N S O O- C16477150 O O O- C41944214 NH O O F F F S C05066222 N H O O N H F F F Cl F F F Cl C02841967 It is not clear how to use this, but probably one can use these information to design compounds have small distance from the known binder at the same time having large distance from the inactive compounds. . RMT Classifier . The distance from the subspace can clearly used as a threshold in a classifier. The author used 95% of the active compound as a threshold. Let&#39;s design a classifier and see if the same works for our dataset. . class RMTClassifier: def __init__(self, mp_threshold_scale=1, train_cutoff=0.95): self.mp_threshold_scale = mp_threshold_scale # scale MP threshold self.train_cutoff = train_cutoff # distance cutoff to contain fraction of train data self.mean = None self.std = None self.column_indices = None self.subspace = None self.cutoff = None def _MP_threshold(self, N, p): &quot;&quot;&quot;return MP threshold based on the shape of input matrix&quot;&quot;&quot; gamma = p / N gamma_p = (1 + sqrt(gamma))**2 return gamma_p def fit(self, train_mat): # remove columns that are all zeros mean = np.mean(train_mat, axis=0) std = np.std(train_mat, axis=0) column_indices = ~((mean == 0) &amp; (std == 0)) train_mat_reduced = train_mat[:, column_indices] # normalize mean = np.mean(train_mat_reduced, axis=0) std = np.std(train_mat_reduced, axis=0) train_mat_normed = (train_mat_reduced - mean) / std # correlation matrix C = np.dot(train_mat_normed.T, train_mat_normed) / train_mat.shape[0] # correlation matrix w, v = np.linalg.eig(C) # w = eigenvalue, v = eigenvetors # MP threshold thres = self._MP_threshold(*train_mat_reduced.shape) * self.mp_threshold_scale # subspace indices = np.argwhere(w &gt; thres).flatten() V = np.real(np.array([v[i] for i in indices])) # determine cutoff train_mat_normed_p = np.dot(np.einsum(&#39;ij,kj-&gt;ki&#39;, V, train_mat_normed), V) dist = np.linalg.norm(train_mat_normed_p - train_mat_normed, axis=1) cutoff = np.percentile(dist, self.train_cutoff * 100) self.column_indices = column_indices self.mean = mean self.std = std self.subspace = V self.cutoff = cutoff def transform(self, test_mat): # normalize test_mat_reduced = test_mat[:, self.column_indices] test_mat_normed = (test_mat_reduced - self.mean) / self.std # determine cutoff test_mat_normed_p = np.dot(np.einsum(&#39;ij,kj-&gt;ki&#39;, self.subspace, test_mat_normed), self.subspace) dist = np.linalg.norm(test_mat_normed_p - test_mat_normed, axis=1) return dist def predict_proba(self, test_mat): dist = self.transform(test_mat) # rescale distance using sigmoid # 0 - cutoff =&gt; close to 1 # cutoff - max =&gt; clost to 0 sigmoid = lambda x: 1 / (1 + np.exp(-x)) return sigmoid(-(dist - model.cutoff)) def predict(self, test_mat): return self.predict_proba(test_mat) &gt; 0.5 . model = RMTClassifier() A_active = np.array(fps_active) model.fit(A_active) . A_decoy = np.array(fps_decoy) pred = model.predict(A_decoy) test_acc = np.sum(~pred) / len(pred) print(test_acc) . 0.9187669585163222 . RMTClassifier returns impressive 91.9% accuracy for the decoy molecule! Let&#39;s draw ROC curve. To make it little more interesting, I&#39;ll mix the active and inactive compounds to make a test dataset. . train_size = 0.1 np.random.seed(1904234) train_indices = np.array([np.random.random() &gt; train_size for i in range(len(A_active))]) train_mat = A_active[train_indices] test_mat = np.concatenate([A_active[~train_indices], A_decoy]) test_target = np.concatenate([[True] * np.sum(~train_indices), [False] * len(A_decoy)]) model = RMTClassifier() model.fit(train_mat) score = model.predict_proba(test_mat) from sklearn import metrics fpr, tpr, threshold = metrics.roc_curve(test_target, score) auc = metrics.roc_auc_score(test_target, score) plt.plot(fpr, tpr) plt.xlabel(&quot;False Positive&quot;) plt.ylabel(&quot;True Positive&quot;) plt.text(x=0.8, y=0.05, s=&quot;AUC = %.2f&quot; % auc) plt.show() . . We get a very impressive AUC of 0.97. Now, I wonder how this method compares with other type of traditional methods, such as Random Forest or SVM. . Comparison with other methods . The direct comparison with other methods such as Random Forest and SVM may not be easy because these requires having a training set that have both positive and negative data. So, the performance data should be taken with some care. . from sklearn import metrics from sklearn.model_selection import StratifiedShuffleSplit from sklearn.ensemble import RandomForestClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC . . Random Forest . X = np.concatenate([A_active, A_decoy]) y = np.concatenate([[True] * len(A_active), [False] * len(A_decoy)]) sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=42) for train_idx, test_idx in tqdm(sss.split(X, y), total=5): X_train = X[train_idx] X_test = X[test_idx] y_train = y[train_idx] y_test = y[test_idx] rf = RandomForestClassifier() rf.fit(X_train, y_train) y_proba = rf.predict_proba(X_test)[:, 1] fpr, tpr, threshold = metrics.roc_curve(y_test, y_proba) auc = metrics.roc_auc_score(y_test, y_proba) print(auc) . 0.9872902228857287 0.981937612948849 0.9906747731633876 0.9961396441758978 0.9940257195987534 . SVM . sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=42) for train_idx, test_idx in tqdm(sss.split(X, y), total=5): X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx] svm = SVC(probability=True) svm.fit(X_train, y_train) y_proba = svm.predict_proba(X_test)[:, 1] fpr, tpr, threshold = metrics.roc_curve(y_test, y_proba) auc = metrics.roc_auc_score(y_test, y_proba) print(auc) . 0.9926975197087556 0.9905676645451926 0.9894833903660616 0.9959659573899553 0.9974714124826485 . Both Random Forest and SVM worked really well on this dataset. This is probably the inactive compounds are synthetic, i.e., match the physico-chemical property but have different topology. This also makes me think that it is no wonder the deep learning based docking scoring function learned to classify decoys instead of really scoring them. It would be really interesting to test this with more challenging dataset. Testing this with method with randomly selected ligands from ChEMBL may not be enough because inactive compounds frequently shares pharamcophore in a live project. . Conclusion . In this post, I have explored random matrix theory and its application in molecular fingerprint classification. It showed impressive AUC in classification task and clear statistical background. When compared with other methods, such as Random Forest or SVM, it showed about the same performance. However, the dataset used in my example was probably too obvious for algorithms to figure out. It was decoy compounds produced for DUD-E benchmark, which were synthetic and clearly have different fingerprint from the active compounds. To compare this method farily, I may have to be revisited with more challenging datast (i.e., classifying non-binders in a live project is harder because the binder/non-binders tend to have similar fingerprint). . Although the method showed similar performance in my current test, the method have an advantage compared to other methods in my opinion. First, RMT classifier provides interpretable model. At the end of classification, you end up with a chemical subspace relevant to the binding to your receptor of interest. You can use this &quot;pharmacophore&quot; to design a next round of compounds or evaluate your current understanding. In the future, it would be interesting to combine distance from the active compounds and from the inactive compounds to derive compound design (i.e., pick compounds close to active yet distant from inactive). . The RMT method, however, does not automatically solve the prospective issue in QSAR. What I mean by &quot;prospective issue&quot; is that many QSAR algorithm can explain what human can perceive already, however, fails to provide new pharamcophore that human could not come up with based on data. This is because the models are derived from the known data and can&#39;t be easily generalized. Though, when the data is very large, a model like this can be useful to design a variation that has a strong potential to be active but simply not tried yet. . The affinity model derived using ising model at the end of the paper seems interesting, but I could not reproduce it in my limited time trying. It looks certainly useful for scafold hopping strategy. .",
            "url": "https://sunhwan.github.io/blog/2021/03/10/Random-Matrix-Theory-Molecular-Classification.html",
            "relUrl": "/2021/03/10/Random-Matrix-Theory-Molecular-Classification.html",
            "date": " • Mar 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Learning Molecular Representation using Graph Neural Network - Training Molecular Graph",
            "content": "Motivation . In the previous post, I have looked into how a molecular graph is constructed and message can be passed around in a MPNN architecture. In this post, I&#39;ll take a look at how the graph neural net can be trained. The details of message passing update will vary by implementation; here we choose what was used in this paper. . Again, many code examples were taken from chemprop repository. The code was initially taken from the chemprop repository and I edited them for the sake of simplicity. . Recap . Let&#39;s briefly recap how the molecular graph is constructed and messages are passed around. As shown in the above figure, each atom and bonds are labeled as $x$ and $e$. Here we are discussing D-MPNN, which represents the graph with a directional edges, which means, for each bond between two atoms $v$ and $w$, there are two directional bond, $e_{vw}$ and $e_{wv}$. . The initial hidden message is constructed as $h_{vw}^0 = tau (W_i mathrm{cat}(x_v, e_{vw}))$ where $W_i$ is a learned matrix and $ tau$ is an activation function. . . Above figure shows two messages, $m_{57}$ and $m_{54}$, are constructed. First, the message from atom $v$ to $w$ is the sum of all the hidden state for the incoming bonds to $v$ (excluding the one originating from $w$). Then the learned matrix $W_m$ is multiplied to the message and the initial hidden message is added to form the new hidden message for the depth 1. This is repeated several times for the message to be passed around to multiple depth. . After the messages are passed up to the given number of depth, the hidden states are summed to be a final message per each atom (all incoming hidden state) and the hidden state for each atom is computed as follows: . $$m_v = sum_{k in N(v)} h_{kv}^t$$ . $$h_v = tau(W_a mathrm{cat} (x_v, m_v))$$ . Finally, the readout phase uses the sum of all $h_v$ to obtain the feature vector of the molecule and property prediction is carried out using a fully-connected feed forward network. . Train Data . As an example, I&#39;ll use Enamine Real&#39;s diversity discovery set composed of 10240 compounds. This dataset contains some molecular properties, such as ClogP and TPSA, so we should be able to train a GCNN that predicts those properties. . For this example, let&#39;s train using ClogP values. . %matplotlib inline import matplotlib.pyplot as plt import matplotlib from io import BytesIO import pandas as pd import numpy as np from IPython.display import SVG # RDKit import rdkit from rdkit.Chem import PandasTools from rdkit import Chem from rdkit.Chem import AllChem from rdkit.Chem import DataStructs from rdkit.Chem import rdMolDescriptors from rdkit.Chem import rdRGroupDecomposition from rdkit.Chem.Draw import IPythonConsole #Needed to show molecules from rdkit.Chem import Draw from rdkit.Chem import rdDepictor from rdkit.Chem.Draw import rdMolDraw2D from rdkit.Chem.Draw.MolDrawing import MolDrawing, DrawingOptions #Only needed if modifying defaults DrawingOptions.bondLineWidth=1.8 IPythonConsole.ipython_useSVG=True from rdkit import RDLogger RDLogger.DisableLog(&#39;rdApp.warning&#39;) print(rdkit.__version__) # pytorch import torch from torch.utils.data import DataLoader, Dataset, Sampler from torch import nn # misc from typing import Dict, Iterator, List, Optional, Union, OrderedDict, Tuple from tqdm.notebook import tqdm from functools import reduce . . 2020.03.2 . # we will define a class which holds various parameter for D-MPNN class TrainArgs: smiles_column = None no_cuda = False gpu = None num_workers = 8 batch_size = 50 no_cache_mol = False dataset_type = &#39;regression&#39; task_names = [] seed = 0 hidden_size = 300 bias = False depth = 3 dropout = 0.0 undirected = False aggregation = &#39;mean&#39; aggregation_norm = 100 ffn_num_layers = 2 ffn_hidden_size = 300 init_lr = 1e-4 max_lr = 1e-3 final_lr = 1e-4 num_lrs = 1 warmup_epochs = 2.0 epochs = 30 @property def device(self) -&gt; torch.device: &quot;&quot;&quot;The :code:`torch.device` on which to load and process data and models.&quot;&quot;&quot; if not self.cuda: return torch.device(&#39;cpu&#39;) return torch.device(&#39;cuda&#39;, self.gpu) @device.setter def device(self, device: torch.device) -&gt; None: self.cuda = device.type == &#39;cuda&#39; self.gpu = device.index @property def cuda(self) -&gt; bool: &quot;&quot;&quot;Whether to use CUDA (i.e., GPUs) or not.&quot;&quot;&quot; return not self.no_cuda and torch.cuda.is_available() @cuda.setter def cuda(self, cuda: bool) -&gt; None: self.no_cuda = not cuda . . args = TrainArgs() args.data_path = &#39;files/enamine_discovery_diversity_set_10240.csv&#39; args.target_column = &#39;ClogP&#39; args.smiles_column = &#39;SMILES&#39; args.dataset_type = &#39;regression&#39; args.task_names = [args.target_column] args.num_tasks = 1 . df = pd.read_csv(args.data_path) df.head() . Name SMILES Catalog ID PlateID Well MW (desalted) ClogP HBD TPSA RotBonds . 0 NaN | CN(C(=O)NC1CCOc2ccccc21)C(c1ccccc1)c1ccccn1 | Z447596076 | 1186474-R-001 | A02 | 373.448 | 2.419 | 1 | 54.46 | 4 | . 1 NaN | Cn1cc(C(=O)N2CCC(OC3CCOC3)CC2)c(C2CC2)n1 | Z2180753156 | 1186474-R-001 | A03 | 319.399 | -0.570 | 0 | 56.59 | 4 | . 2 NaN | CC(=O)N(C)C1CCN(C(=O)c2ccccc2-c2ccccc2C(=O)O)CC1 | Z2295858832 | 1186474-R-001 | A04 | 380.437 | 0.559 | 1 | 77.92 | 4 | . 3 NaN | COCC1(CNc2cnccc2C#N)CCNCC1 | Z2030994006 | 1186474-R-001 | A05 | 260.335 | 0.902 | 2 | 69.97 | 5 | . 4 NaN | CCCCOc1ccc(-c2nnc3n2CCCC3)cc1OC | Z273627850 | 1186474-R-001 | A06 | 301.383 | 3.227 | 0 | 49.17 | 6 | . from random import Random # Cache of graph featurizations CACHE_GRAPH = True SMILES_TO_GRAPH = {} def cache_graph(): return CACHE_GRAPH def set_cache_graph(cache_graph): global CACHE_GRAPH CACHE_GRAPH = cache_graph # Cache of RDKit molecules CACHE_MOL = True SMILES_TO_MOL: Dict[str, Chem.Mol] = {} def cache_mol() -&gt; bool: r&quot;&quot;&quot;Returns whether RDKit molecules will be cached.&quot;&quot;&quot; return CACHE_MOL def set_cache_mol(cache_mol: bool) -&gt; None: r&quot;&quot;&quot;Sets whether RDKit molecules will be cached.&quot;&quot;&quot; global CACHE_MOL CACHE_MOL = cache_mol # Atom feature sizes MAX_ATOMIC_NUM = 100 ATOM_FEATURES = { &#39;atomic_num&#39;: list(range(MAX_ATOMIC_NUM)), &#39;degree&#39;: [0, 1, 2, 3, 4, 5], &#39;formal_charge&#39;: [-1, -2, 1, 2, 0], &#39;chiral_tag&#39;: [0, 1, 2, 3], &#39;num_Hs&#39;: [0, 1, 2, 3, 4], &#39;hybridization&#39;: [ Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2 ], } # Distance feature sizes PATH_DISTANCE_BINS = list(range(10)) THREE_D_DISTANCE_MAX = 20 THREE_D_DISTANCE_STEP = 1 THREE_D_DISTANCE_BINS = list(range(0, THREE_D_DISTANCE_MAX + 1, THREE_D_DISTANCE_STEP)) # len(choices) + 1 to include room for uncommon values; + 2 at end for IsAromatic and mass ATOM_FDIM = sum(len(choices) + 1 for choices in ATOM_FEATURES.values()) + 2 EXTRA_ATOM_FDIM = 0 BOND_FDIM = 14 def get_atom_fdim(): &quot;&quot;&quot;Gets the dimensionality of the atom feature vector.&quot;&quot;&quot; return ATOM_FDIM + EXTRA_ATOM_FDIM def get_bond_fdim(atom_messages=False): &quot;&quot;&quot;Gets the dimensionality of the bond feature vector. &quot;&quot;&quot; return BOND_FDIM + (not atom_messages) * get_atom_fdim() def onek_encoding_unk(value: int, choices: List[int]): encoding = [0] * (len(choices) + 1) index = choices.index(value) if value in choices else -1 encoding[index] = 1 return encoding def atom_features(atom: Chem.rdchem.Atom, functional_groups: List[int] = None): &quot;&quot;&quot;Builds a feature vector for an atom. &quot;&quot;&quot; features = onek_encoding_unk(atom.GetAtomicNum() - 1, ATOM_FEATURES[&#39;atomic_num&#39;]) + onek_encoding_unk(atom.GetTotalDegree(), ATOM_FEATURES[&#39;degree&#39;]) + onek_encoding_unk(atom.GetFormalCharge(), ATOM_FEATURES[&#39;formal_charge&#39;]) + onek_encoding_unk(int(atom.GetChiralTag()), ATOM_FEATURES[&#39;chiral_tag&#39;]) + onek_encoding_unk(int(atom.GetTotalNumHs()), ATOM_FEATURES[&#39;num_Hs&#39;]) + onek_encoding_unk(int(atom.GetHybridization()), ATOM_FEATURES[&#39;hybridization&#39;]) + [1 if atom.GetIsAromatic() else 0] + [atom.GetMass() * 0.01] # scaled to about the same range as other features if functional_groups is not None: features += functional_groups return features def bond_features(bond: Chem.rdchem.Bond): &quot;&quot;&quot;Builds a feature vector for a bond. &quot;&quot;&quot; if bond is None: fbond = [1] + [0] * (BOND_FDIM - 1) else: bt = bond.GetBondType() fbond = [ 0, # bond is not None bt == Chem.rdchem.BondType.SINGLE, bt == Chem.rdchem.BondType.DOUBLE, bt == Chem.rdchem.BondType.TRIPLE, bt == Chem.rdchem.BondType.AROMATIC, (bond.GetIsConjugated() if bt is not None else 0), (bond.IsInRing() if bt is not None else 0) ] fbond += onek_encoding_unk(int(bond.GetStereo()), list(range(6))) return fbond class MoleculeDatapoint: def __init__(self, smiles: str, targets: List[Optional[float]] = None, row: OrderedDict = None): self.smiles = smiles self.targets = targets self.features = [] self.row = row @property def mol(self) -&gt; Chem.Mol: &quot;&quot;&quot;Gets the corresponding list of RDKit molecules for the corresponding SMILES.&quot;&quot;&quot; mol = SMILES_TO_MOL.get(self.smiles, Chem.MolFromSmiles(self.smiles)) if cache_mol(): SMILES_TO_MOL[self.smiles] = mol return mol def set_features(self, features: np.ndarray) -&gt; None: &quot;&quot;&quot;Sets the features of the molecule. &quot;&quot;&quot; self.features = features def extend_features(self, features: np.ndarray) -&gt; None: &quot;&quot;&quot;Extends the features of the molecule. &quot;&quot;&quot; self.features = np.append(self.features, features) if self.features is not None else features def num_tasks(self) -&gt; int: &quot;&quot;&quot;Returns the number of prediction tasks. &quot;&quot;&quot; return len(self.targets) def set_targets(self, targets: List[Optional[float]]): &quot;&quot;&quot;Sets the targets of a molecule. &quot;&quot;&quot; self.targets = targets def reset_features_and_targets(self) -&gt; None: &quot;&quot;&quot;Resets the features and targets to their raw values.&quot;&quot;&quot; self.features, self.targets = self.raw_features, self.raw_targets class MoleculeDataset(Dataset): def __init__(self, data: List[MoleculeDatapoint]): self._data = data self._scaler = None self._batch_graph = None self._random = Random() def smiles(self) -&gt; List[str]: return [d.smiles for d in self._data] def mols(self) -&gt; List[Chem.Mol]: return [d.mol for d in self._data] def targets(self) -&gt; List[List[Optional[float]]]: return [d.targets for d in self._data] def num_tasks(self) -&gt; int: return self._data[0].num_tasks() if len(self._data) &gt; 0 else None def set_targets(self, targets: List[List[Optional[float]]]) -&gt; None: assert len(self._data) == len(targets) for i in range(len(self._data)): self._data[i].set_targets(targets[i]) def reset_features_and_targets(self) -&gt; None: for d in self._data: d.reset_features_and_targets() def __len__(self) -&gt; int: return len(self._data) def __getitem__(self, item) -&gt; Union[MoleculeDatapoint, List[MoleculeDatapoint]]: return self._data[item] def batch_graph(self): if self._batch_graph is None: self._batch_graph = [] mol_graphs = [] for d in self._data: mol_graphs_list = [] if d.smiles in SMILES_TO_GRAPH: mol_graph = SMILES_TO_GRAPH[d.smiles] else: mol_graph = MolGraph(d.mol) if cache_graph(): SMILES_TO_GRAPH[d.smiles] = mol_graph mol_graphs.append([mol_graph]) self._batch_graph = [BatchMolGraph([g[i] for g in mol_graphs]) for i in range(len(mol_graphs[0]))] return self._batch_graph def features(self) -&gt; List[np.ndarray]: &quot;&quot;&quot; Returns the features associated with each molecule (if they exist). :return: A list of 1D numpy arrays containing the features for each molecule or None if there are no features. &quot;&quot;&quot; if len(self._data) == 0 or self._data[0].features is None: return None return [d.features for d in self._data] def index_select_ND(source: torch.Tensor, index: torch.Tensor) -&gt; torch.Tensor: &quot;&quot;&quot;Selects the message features from source corresponding to the atom or bond indices in index. &quot;&quot;&quot; index_size = index.size() # (num_atoms/num_bonds, max_num_bonds) suffix_dim = source.size()[1:] # (hidden_size,) final_size = index_size + suffix_dim # (num_atoms/num_bonds, max_num_bonds, hidden_size) target = source.index_select(dim=0, index=index.view(-1)) # (num_atoms/num_bonds * max_num_bonds, hidden_size) target = target.view(final_size) # (num_atoms/num_bonds, max_num_bonds, hidden_size) return target class MolGraph: def __init__(self, mol, atom_descriptors=None): # Convert SMILES to RDKit molecule if necessary if type(mol) == str: mol = Chem.MolFromSmiles(mol) self.n_atoms = 0 # number of atoms self.n_bonds = 0 # number of bonds self.f_atoms = [] # mapping from atom index to atom features self.f_bonds = [] # mapping from bond index to concat(in_atom, bond) features self.a2b = [] # mapping from atom index to incoming bond indices self.b2a = [] # mapping from bond index to the index of the atom the bond is coming from self.b2revb = [] # mapping from bond index to the index of the reverse bond # Get atom features self.f_atoms = [atom_features(atom) for atom in mol.GetAtoms()] if atom_descriptors is not None: self.f_atoms = [f_atoms + descs.tolist() for f_atoms, descs in zip(self.f_atoms, atom_descriptors)] self.n_atoms = len(self.f_atoms) # Initialize atom to bond mapping for each atom for _ in range(self.n_atoms): self.a2b.append([]) # Get bond features for a1 in range(self.n_atoms): for a2 in range(a1 + 1, self.n_atoms): bond = mol.GetBondBetweenAtoms(a1, a2) if bond is None: continue f_bond = bond_features(bond) self.f_bonds.append(self.f_atoms[a1] + f_bond) self.f_bonds.append(self.f_atoms[a2] + f_bond) # Update index mappings b1 = self.n_bonds b2 = b1 + 1 self.a2b[a2].append(b1) # b1 = a1 --&gt; a2 self.b2a.append(a1) self.a2b[a1].append(b2) # b2 = a2 --&gt; a1 self.b2a.append(a2) self.b2revb.append(b2) self.b2revb.append(b1) self.n_bonds += 2 class BatchMolGraph: &quot;&quot;&quot;A `BatchMolGraph` represents the graph structure and featurization of a batch of molecules. &quot;&quot;&quot; def __init__(self, mol_graphs: List[MolGraph]): self.atom_fdim = get_atom_fdim() self.bond_fdim = get_bond_fdim() # Start n_atoms and n_bonds at 1 b/c zero padding self.n_atoms = 1 # number of atoms (start at 1 b/c need index 0 as padding) self.n_bonds = 1 # number of bonds (start at 1 b/c need index 0 as padding) self.a_scope = [] # list of tuples indicating (start_atom_index, num_atoms) for each molecule self.b_scope = [] # list of tuples indicating (start_bond_index, num_bonds) for each molecule # All start with zero padding so that indexing with zero padding returns zeros f_atoms = [[0] * self.atom_fdim] # atom features f_bonds = [[0] * self.bond_fdim] # combined atom/bond features a2b = [[]] # mapping from atom index to incoming bond indices b2a = [0] # mapping from bond index to the index of the atom the bond is coming from b2revb = [0] # mapping from bond index to the index of the reverse bond for mol_graph in mol_graphs: f_atoms.extend(mol_graph.f_atoms) f_bonds.extend(mol_graph.f_bonds) for a in range(mol_graph.n_atoms): a2b.append([b + self.n_bonds for b in mol_graph.a2b[a]]) for b in range(mol_graph.n_bonds): b2a.append(self.n_atoms + mol_graph.b2a[b]) b2revb.append(self.n_bonds + mol_graph.b2revb[b]) self.a_scope.append((self.n_atoms, mol_graph.n_atoms)) self.b_scope.append((self.n_bonds, mol_graph.n_bonds)) self.n_atoms += mol_graph.n_atoms self.n_bonds += mol_graph.n_bonds self.max_num_bonds = max(1, max(len(in_bonds) for in_bonds in a2b)) # max with 1 to fix a crash in rare case of all single-heavy-atom mols self.f_atoms = torch.FloatTensor(f_atoms) self.f_bonds = torch.FloatTensor(f_bonds) self.a2b = torch.LongTensor([a2b[a] + [0] * (self.max_num_bonds - len(a2b[a])) for a in range(self.n_atoms)]) self.b2a = torch.LongTensor(b2a) self.b2revb = torch.LongTensor(b2revb) self.b2b = None # try to avoid computing b2b b/c O(n_atoms^3) self.a2a = None # only needed if using atom messages def get_components(self, atom_messages: bool = False) -&gt; Tuple[torch.FloatTensor, torch.FloatTensor, torch.LongTensor, torch.LongTensor, torch.LongTensor, List[Tuple[int, int]], List[Tuple[int, int]]]: return self.f_atoms, self.f_bonds, self.a2b, self.b2a, self.b2revb, self.a_scope, self.b_scope def get_b2b(self) -&gt; torch.LongTensor: &quot;&quot;&quot;Computes (if necessary) and returns a mapping from each bond index to all the incoming bond indices. &quot;&quot;&quot; if self.b2b is None: b2b = self.a2b[self.b2a] # num_bonds x max_num_bonds # b2b includes reverse edge for each bond so need to mask out revmask = (b2b != self.b2revb.unsqueeze(1).repeat(1, b2b.size(1))).long() # num_bonds x max_num_bonds self.b2b = b2b * revmask return self.b2b def get_a2a(self) -&gt; torch.LongTensor: &quot;&quot;&quot;Computes (if necessary) and returns a mapping from each atom index to all neighboring atom indices. &quot;&quot;&quot; if self.a2a is None: # b = a1 --&gt; a2 # a2b maps a2 to all incoming bonds b # b2a maps each bond b to the atom it comes from a1 # thus b2a[a2b] maps atom a2 to neighboring atoms a1 self.a2a = self.b2a[self.a2b] # num_atoms x max_num_bonds return self.a2a . . # prepare data set data = MoleculeDataset([ MoleculeDatapoint( smiles=row[args.smiles_column], targets=[row[args.target_column]] ) for i, row in df.iterrows() ]) # split data into train, validation and test set random = Random() sizes = [0.8, 0.1, 0.1] indices = list(range(len(data))) random.shuffle(indices) train_size = int(sizes[0] * len(data)) train_val_size = int((sizes[0] + sizes[1]) * len(data)) train = [data[i] for i in indices[:train_size]] val = [data[i] for i in indices[train_size:train_val_size]] test = [data[i] for i in indices[train_val_size:]] train_data = MoleculeDataset(train) val_data = MoleculeDataset(val) test_data = MoleculeDataset(test) . NameError Traceback (most recent call last) &lt;ipython-input-1-018356360359&gt; in &lt;module&gt; 1 # prepare data set -&gt; 2 data = MoleculeDataset([ 3 MoleculeDatapoint( 4 smiles=row[args.smiles_column], 5 targets=[row[args.target_column]] NameError: name &#39;MoleculeDataset&#39; is not defined . MPNN Model . Let&#39;s create a MPNN model. The model is composed of encoder and feed-forward network (FFN). The encoder is same as the one we discussed before and the FFN is defined as a straightforward neural network. . # Atom feature sizes MAX_ATOMIC_NUM = 100 ATOM_FEATURES = { &#39;atomic_num&#39;: list(range(MAX_ATOMIC_NUM)), &#39;degree&#39;: [0, 1, 2, 3, 4, 5], &#39;formal_charge&#39;: [-1, -2, 1, 2, 0], &#39;chiral_tag&#39;: [0, 1, 2, 3], &#39;num_Hs&#39;: [0, 1, 2, 3, 4], &#39;hybridization&#39;: [ Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2 ], } # Distance feature sizes PATH_DISTANCE_BINS = list(range(10)) THREE_D_DISTANCE_MAX = 20 THREE_D_DISTANCE_STEP = 1 THREE_D_DISTANCE_BINS = list(range(0, THREE_D_DISTANCE_MAX + 1, THREE_D_DISTANCE_STEP)) # len(choices) + 1 to include room for uncommon values; + 2 at end for IsAromatic and mass ATOM_FDIM = sum(len(choices) + 1 for choices in ATOM_FEATURES.values()) + 2 EXTRA_ATOM_FDIM = 0 BOND_FDIM = 14 def get_atom_fdim() -&gt; int: &quot;&quot;&quot;Gets the dimensionality of the atom feature vector.&quot;&quot;&quot; return ATOM_FDIM + EXTRA_ATOM_FDIM def get_bond_fdim() -&gt; int: &quot;&quot;&quot;Gets the dimensionality of the bond feature vector. &quot;&quot;&quot; return BOND_FDIM + get_atom_fdim() def onek_encoding_unk(value: int, choices: List[int]) -&gt; List[int]: encoding = [0] * (len(choices) + 1) index = choices.index(value) if value in choices else -1 encoding[index] = 1 return encoding def atom_features(atom: Chem.rdchem.Atom, functional_groups: List[int] = None) -&gt; List[Union[bool, int, float]]: &quot;&quot;&quot;Builds a feature vector for an atom. &quot;&quot;&quot; features = onek_encoding_unk(atom.GetAtomicNum() - 1, ATOM_FEATURES[&#39;atomic_num&#39;]) + onek_encoding_unk(atom.GetTotalDegree(), ATOM_FEATURES[&#39;degree&#39;]) + onek_encoding_unk(atom.GetFormalCharge(), ATOM_FEATURES[&#39;formal_charge&#39;]) + onek_encoding_unk(int(atom.GetChiralTag()), ATOM_FEATURES[&#39;chiral_tag&#39;]) + onek_encoding_unk(int(atom.GetTotalNumHs()), ATOM_FEATURES[&#39;num_Hs&#39;]) + onek_encoding_unk(int(atom.GetHybridization()), ATOM_FEATURES[&#39;hybridization&#39;]) + [1 if atom.GetIsAromatic() else 0] + [atom.GetMass() * 0.01] # scaled to about the same range as other features if functional_groups is not None: features += functional_groups return features def initialize_weights(model: nn.Module) -&gt; None: &quot;&quot;&quot;Initializes the weights of a model in place. &quot;&quot;&quot; for param in model.parameters(): if param.dim() == 1: nn.init.constant_(param, 0) else: nn.init.xavier_normal_(param) class MPNEncoder(nn.Module): def __init__(self, args, atom_fdim, bond_fdim): super(MPNEncoder, self).__init__() self.atom_fdim = atom_fdim self.bond_fdim = bond_fdim self.hidden_size = args.hidden_size self.bias = args.bias self.depth = args.depth self.dropout = args.dropout self.layers_per_message = 1 self.undirected = False self.atom_messages = False self.device = args.device self.aggregation = args.aggregation self.aggregation_norm = args.aggregation_norm self.dropout_layer = nn.Dropout(p=self.dropout) self.act_func = nn.ReLU() self.cached_zero_vector = nn.Parameter(torch.zeros(self.hidden_size), requires_grad=False) # Input input_dim = self.bond_fdim self.W_i = nn.Linear(input_dim, self.hidden_size, bias=self.bias) w_h_input_size = self.hidden_size # Shared weight matrix across depths (default) self.W_h = nn.Linear(w_h_input_size, self.hidden_size, bias=self.bias) self.W_o = nn.Linear(self.atom_fdim + self.hidden_size, self.hidden_size) def forward(self, mol_graph): &quot;&quot;&quot;Encodes a batch of molecular graphs. &quot;&quot;&quot; f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope = mol_graph.get_components() f_atoms, f_bonds, a2b, b2a, b2revb = f_atoms.to(self.device), f_bonds.to(self.device), a2b.to(self.device), b2a.to(self.device), b2revb.to(self.device) input = self.W_i(f_bonds) # num_bonds x hidden_size message = self.act_func(input) # num_bonds x hidden_size # Message passing for depth in range(self.depth - 1): # m(a1 -&gt; a2) = [sum_{a0 in nei(a1)} m(a0 -&gt; a1)] - m(a2 -&gt; a1) # message a_message = sum(nei_a_message) rev_message nei_a_message = index_select_ND(message, a2b) # num_atoms x max_num_bonds x hidden a_message = nei_a_message.sum(dim=1) # num_atoms x hidden rev_message = message[b2revb] # num_bonds x hidden message = a_message[b2a] - rev_message # num_bonds x hidden message = self.W_h(message) message = self.act_func(input + message) # num_bonds x hidden_size message = self.dropout_layer(message) # num_bonds x hidden a2x = a2b nei_a_message = index_select_ND(message, a2x) # num_atoms x max_num_bonds x hidden a_message = nei_a_message.sum(dim=1) # num_atoms x hidden a_input = torch.cat([f_atoms, a_message], dim=1) # num_atoms x (atom_fdim + hidden) atom_hiddens = self.act_func(self.W_o(a_input)) # num_atoms x hidden atom_hiddens = self.dropout_layer(atom_hiddens) # num_atoms x hidden # Readout mol_vecs = [] for i, (a_start, a_size) in enumerate(a_scope): if a_size == 0: mol_vecs.append(self.cached_zero_vector) else: cur_hiddens = atom_hiddens.narrow(0, a_start, a_size) mol_vec = cur_hiddens # (num_atoms, hidden_size) if self.aggregation == &#39;mean&#39;: mol_vec = mol_vec.sum(dim=0) / a_size elif self.aggregation == &#39;sum&#39;: mol_vec = mol_vec.sum(dim=0) elif self.aggregation == &#39;norm&#39;: mol_vec = mol_vec.sum(dim=0) / self.aggregation_norm mol_vecs.append(mol_vec) mol_vecs = torch.stack(mol_vecs, dim=0) # (num_molecules, hidden_size) return mol_vecs # num_molecules x hidden class MPN(nn.Module): def __init__(self, args, atom_fdim=None, bond_fdim=None): super(MPN, self).__init__() self.atom_fdim = atom_fdim or get_atom_fdim() self.bond_fdim = bond_fdim or get_bond_fdim() self.device = args.device self.encoder = MPNEncoder(args, self.atom_fdim, self.bond_fdim) def forward(self, batch): &quot;&quot;&quot;Encodes a batch of molecules. &quot;&quot;&quot; if type(batch[0]) != BatchMolGraph: batch = [mol2graph(b) for b in batch] encodings = [self.encoder(batch[0])] output = reduce(lambda x, y: torch.cat((x, y), dim=1), encodings) return output class MoleculeModel(nn.Module): def __init__(self, args, featurizer=False): super(MoleculeModel, self).__init__() self.classification = args.dataset_type == &#39;classification&#39; self.featurizer = featurizer self.output_size = args.num_tasks if self.classification: self.sigmoid = nn.Sigmoid() self.create_encoder(args) self.create_ffn(args) initialize_weights(self) def create_encoder(self, args): self.encoder = MPN(args) def create_ffn(self, args): first_linear_dim = args.hidden_size dropout = nn.Dropout(args.dropout) activation = nn.ReLU() # Create FFN layers if args.ffn_num_layers == 1: ffn = [ dropout, nn.Linear(first_linear_dim, self.output_size) ] else: ffn = [ dropout, nn.Linear(first_linear_dim, args.ffn_hidden_size) ] for _ in range(args.ffn_num_layers - 2): ffn.extend([ activation, dropout, nn.Linear(args.ffn_hidden_size, args.ffn_hidden_size), ]) ffn.extend([ activation, dropout, nn.Linear(args.ffn_hidden_size, self.output_size), ]) # Create FFN model self.ffn = nn.Sequential(*ffn) def featurize(self, batch, features_batch=None, atom_descriptors_batch=None): &quot;&quot;&quot;Computes feature vectors of the input by running the model except for the last layer. &quot;&quot;&quot; return self.ffn[:-1](self.encoder(batch, features_batch, atom_descriptors_batch)) def forward(self, batch): output = self.ffn(self.encoder(batch)) # Don&#39;t apply sigmoid during training b/c using BCEWithLogitsLoss if self.classification and not self.training: output = self.sigmoid(output) return output . . model = MoleculeModel(args) model = model.to(args.device) . As it is shown below, the model is comprised of an encoder and FFN. The encoder has three learned matrices and the FFN has 2 fully-connected layers. . model . MoleculeModel( (encoder): MPN( (encoder): MPNEncoder( (dropout_layer): Dropout(p=0.0, inplace=False) (act_func): ReLU() (W_i): Linear(in_features=147, out_features=300, bias=False) (W_h): Linear(in_features=300, out_features=300, bias=False) (W_o): Linear(in_features=433, out_features=300, bias=True) ) ) (ffn): Sequential( (0): Dropout(p=0.0, inplace=False) (1): Linear(in_features=300, out_features=300, bias=True) (2): ReLU() (3): Dropout(p=0.0, inplace=False) (4): Linear(in_features=300, out_features=1, bias=True) ) ) . Train the MPNN . from torch.optim.lr_scheduler import _LRScheduler from torch.optim import Adam, Optimizer class NoamLR(_LRScheduler): &quot;&quot;&quot; Noam learning rate scheduler with piecewise linear increase and exponential decay. The learning rate increases linearly from init_lr to max_lr over the course of the first warmup_steps (where :code:`warmup_steps = warmup_epochs * steps_per_epoch`). Then the learning rate decreases exponentially from :code:`max_lr` to :code:`final_lr` over the course of the remaining :code:`total_steps - warmup_steps` (where :code:`total_steps = total_epochs * steps_per_epoch`). This is roughly based on the learning rate schedule from `Attention is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;`_, section 5.3. &quot;&quot;&quot; def __init__(self, optimizer: Optimizer, warmup_epochs: List[Union[float, int]], total_epochs: List[int], steps_per_epoch: int, init_lr: List[float], max_lr: List[float], final_lr: List[float]): assert len(optimizer.param_groups) == len(warmup_epochs) == len(total_epochs) == len(init_lr) == len(max_lr) == len(final_lr) self.num_lrs = len(optimizer.param_groups) self.optimizer = optimizer self.warmup_epochs = np.array(warmup_epochs) self.total_epochs = np.array(total_epochs) self.steps_per_epoch = steps_per_epoch self.init_lr = np.array(init_lr) self.max_lr = np.array(max_lr) self.final_lr = np.array(final_lr) self.current_step = 0 self.lr = init_lr self.warmup_steps = (self.warmup_epochs * self.steps_per_epoch).astype(int) self.total_steps = self.total_epochs * self.steps_per_epoch self.linear_increment = (self.max_lr - self.init_lr) / self.warmup_steps self.exponential_gamma = (self.final_lr / self.max_lr) ** (1 / (self.total_steps - self.warmup_steps)) super(NoamLR, self).__init__(optimizer) def get_lr(self) -&gt; List[float]: return list(self.lr) def step(self, current_step: int = None): if current_step is not None: self.current_step = current_step else: self.current_step += 1 for i in range(self.num_lrs): if self.current_step &lt;= self.warmup_steps[i]: self.lr[i] = self.init_lr[i] + self.current_step * self.linear_increment[i] elif self.current_step &lt;= self.total_steps[i]: self.lr[i] = self.max_lr[i] * (self.exponential_gamma[i] ** (self.current_step - self.warmup_steps[i])) else: # theoretically this case should never be reached since training should stop at total_steps self.lr[i] = self.final_lr[i] self.optimizer.param_groups[i][&#39;lr&#39;] = self.lr[i] . . import threading def construct_molecule_batch(data): data = MoleculeDataset(data) data.batch_graph() # Forces computation and caching of the BatchMolGraph for the molecules return data class MoleculeSampler(Sampler): def __init__(self, dataset, shuffle=False, seed=0): super(Sampler, self).__init__() self.dataset = dataset self.shuffle = shuffle self._random = Random(seed) self.positive_indices = self.negative_indices = None self.length = len(self.dataset) def __iter__(self): indices = list(range(len(self.dataset))) if self.shuffle: self._random.shuffle(indices) return iter(indices) def __len__(self): return self.length class MoleculeDataLoader(DataLoader): def __init__(self, dataset: MoleculeDataset, batch_size: int = 50, num_workers: int = 8, shuffle: bool = False, seed: int = 0): self._dataset = dataset self._batch_size = batch_size self._num_workers = num_workers self._shuffle = shuffle self._seed = seed self._context = None self._class_balance = False self._timeout = 0 is_main_thread = threading.current_thread() is threading.main_thread() if not is_main_thread and self._num_workers &gt; 0: self._context = &#39;forkserver&#39; # In order to prevent a hanging self._timeout = 3600 # Just for sure that the DataLoader won&#39;t hang self._sampler = MoleculeSampler( dataset=self._dataset, shuffle=self._shuffle, seed=self._seed ) super(MoleculeDataLoader, self).__init__( dataset=self._dataset, batch_size=self._batch_size, sampler=self._sampler, num_workers=self._num_workers, collate_fn=construct_molecule_batch, multiprocessing_context=self._context, timeout=self._timeout ) @property def targets(self) -&gt; List[List[Optional[float]]]: if self._class_balance or self._shuffle: raise ValueError(&#39;Cannot safely extract targets when class balance or shuffle are enabled.&#39;) return [self._dataset[index].targets for index in self._sampler] @property def iter_size(self) -&gt; int: return len(self._sampler) def __iter__(self) -&gt; Iterator[MoleculeDataset]: return super(MoleculeDataLoader, self).__iter__() . . # Create data loaders train_data_loader = MoleculeDataLoader( dataset=train_data, batch_size=args.batch_size, num_workers=8, shuffle=True, seed=args.seed ) val_data_loader = MoleculeDataLoader( dataset=val_data, batch_size=args.batch_size, num_workers=8 ) test_data_loader = MoleculeDataLoader( dataset=test_data, batch_size=args.batch_size, num_workers=8 ) . # optimizer params = [{&#39;params&#39;: model.parameters(), &#39;lr&#39;: args.init_lr, &#39;weight_decay&#39;: 0}] optimizer = Adam(params) # scheduler scheduler = NoamLR( optimizer=optimizer, warmup_epochs=[args.warmup_epochs], total_epochs=[args.epochs] * args.num_lrs, steps_per_epoch=len(train_data) // args.batch_size, init_lr=[args.init_lr], max_lr=[args.max_lr], final_lr=[args.final_lr] ) # loss function loss_func = nn.MSELoss(reduction=&#39;none&#39;) . # train loop model.train() loss_sum = iter_count = 0 n_iter = 0 for batch in tqdm(train_data_loader, total=len(train_data_loader), leave=False): mol_batch, target_batch = batch.batch_graph(), batch.targets() mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch]) targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch]) # Run model model.zero_grad() preds = model(mol_batch) # Move tensors to correct device mask = mask.to(preds.device) targets = targets.to(preds.device) class_weights = torch.ones(targets.shape, device=preds.device) loss = loss_func(preds, targets) * class_weights * mask loss = loss.sum() / mask.sum() loss_sum += loss.item() iter_count += 1 loss.backward() optimizer.step() if isinstance(scheduler, NoamLR): scheduler.step() n_iter += len(batch) . Training for the first epoch is finished. Let&#39;s take a look at how well the model predict the ClogP. . model.eval() initial_preds = [] for batch in tqdm(val_data_loader, disable=False, leave=False): # Prepare batch batch: MoleculeDataset mol_batch = batch.batch_graph() # Make predictions with torch.no_grad(): batch_preds = model(mol_batch) batch_preds = batch_preds.data.cpu().numpy() # Collect vectors batch_preds = batch_preds.tolist() initial_preds.extend(batch_preds) . from sklearn.metrics import mean_squared_error # valid_preds and valid_targets have shape (num_tasks, data_size) targets = val_data_loader.targets metric_func = mean_squared_error valid_preds = [[] for _ in range(args.num_tasks)] valid_targets = [[] for _ in range(args.num_tasks)] for i in range(args.num_tasks): for j in range(len(preds)): if targets[j][i] is not None: # Skip those without targets valid_preds[i].append(preds[j][i].detach()) valid_targets[i].append(targets[j][i]) result = metric_func(valid_targets[i], valid_preds[i]) print(&#39;MSE:&#39;, result) . MSE: 3.0658553721115887 . fit = plt.figure(figsize=(4,4)) plt.scatter(valid_targets[i], valid_preds[i]) plt.xlabel(&#39;Target ClogP&#39;) plt.ylabel(&#39;Predicted ClogP&#39;) plt.show() . Almost no correlation as of first epoch. Let&#39;s train a few more epochs and see if the prediciton improves. . scheduler = NoamLR( optimizer=optimizer, warmup_epochs=[args.warmup_epochs], total_epochs=[args.epochs] * args.num_lrs, steps_per_epoch=len(train_data) // args.batch_size, init_lr=[args.init_lr], max_lr=[args.max_lr], final_lr=[args.final_lr] ) loss_func = nn.MSELoss(reduction=&#39;none&#39;) optimizer = Adam(params) metric_func = mean_squared_error for epoch in tqdm(range(args.epochs)): # train model.train() loss_sum = iter_count = 0 n_iter = 0 for batch in tqdm(train_data_loader, total=len(train_data_loader), leave=False): mol_batch, target_batch = batch.batch_graph(), batch.targets() mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch]) targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch]) # Run model model.zero_grad() preds = model(mol_batch) # Move tensors to correct device mask = mask.to(preds.device) targets = targets.to(preds.device) class_weights = torch.ones(targets.shape, device=preds.device) loss = loss_func(preds, targets) * class_weights * mask loss = loss.sum() / mask.sum() loss_sum += loss.item() iter_count += 1 loss.backward() optimizer.step() if isinstance(scheduler, NoamLR): scheduler.step() n_iter += len(batch) # eval model.eval() preds = [] for batch in tqdm(val_data_loader, disable=False, leave=False): # Prepare batch batch: MoleculeDataset mol_batch = batch.batch_graph() # Make predictions with torch.no_grad(): batch_preds = model(mol_batch) batch_preds = batch_preds.data.cpu().numpy() # Collect vectors batch_preds = batch_preds.tolist() preds.extend(batch_preds) # valid_preds and valid_targets have shape (num_tasks, data_size) num_tasks = 1 targets = val_data_loader.targets valid_preds = [[] for _ in range(num_tasks)] valid_targets = [[] for _ in range(num_tasks)] for i in range(num_tasks): for j in range(len(preds)): if targets[j][i] is not None: # Skip those without targets valid_preds[i].append(preds[j][i]) valid_targets[i].append(targets[j][i]) result = metric_func(valid_targets[i], valid_preds[i]) print(epoch, result) . 0 0.5059666700564436 1 0.6747574089163861 2 0.29365952162247105 3 0.2443422001168768 4 0.22893787807471938 5 0.24693692581926335 6 0.18068380381421822 7 0.16628313459548472 8 0.15646424430563233 9 0.14714968670153764 10 0.1484021422019028 11 0.1497949169195903 12 0.1332405691696682 13 0.12293908860736892 14 0.12480342381848945 15 0.12231600820223316 16 0.12469534214189118 17 0.12742151396520932 18 0.10947115821963195 19 0.11363330373825292 20 0.10276127977714987 21 0.10135583119599778 22 0.10164295643187546 23 0.09673642613524305 24 0.09645447176601984 25 0.0980465410170695 26 0.09245372955020192 27 0.08906743111716783 28 0.0879800185507161 29 0.09138532813108272 . Over the course of training 30 epochs, the MSE quickly improved and reached to less than 0.1 at the end! Let&#39;s take a look at the target vs predicted values. So, we got to this level of prediction without ever specifically wrote code for computing ClogP. . fit = plt.figure(figsize=(4,4)) plt.scatter(valid_targets[i], valid_preds[i]) plt.xlabel(&#39;Target ClogP&#39;) plt.ylabel(&#39;Predicted ClogP&#39;) plt.show() . Conclusion . In this post, I have trained a graph neural network that can predict ClogP property. Within 30 epochs, it was able to predict the property pretty accurately in less than 0.1 MSE. Given only very simple features were used in atom and bond features, it was able to &quot;learn&quot; to predict the property fairly quickly. . Now that we have a trained model, a few things I&#39;d like to try: . compare this model with other traditional model and compare performance | try different parameters, such as depth | try alternative featurization, i.e., add if bond is rotatable in the bond_features and so on. | add long-range connection;current network is limited to chemical bonds, but longer range interaction may also be important. | .",
            "url": "https://sunhwan.github.io/blog/2021/03/07/Learning-Molecular-Representation-Using-Graph-Neural-Network-Training-Molecular-Graph.html",
            "relUrl": "/2021/03/07/Learning-Molecular-Representation-Using-Graph-Neural-Network-Training-Molecular-Graph.html",
            "date": " • Mar 7, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Piperazine Ring Conformation using RDKit",
            "content": "Motivation . I noticed whenever I built 3D conformers of molecules containing piperazine (or cyclohexane) using RDKit, I tend to get a distorted ring conformation. RDKit&#39;s ETKDG (Experimental Torsion angle Knowledge-based Distance Geometry) algorithm works really well in general, but, in this case, it was not doing a good job at coming up with a reasonable initial conformation. I wanted to first quantify how much chair vs boat vs twisted conformer I get, so either I could use it to filter out non-desirable conformers or improve the RDKit&#39;s conformer generation routine. . %matplotlib inline import matplotlib.pyplot as plt from io import BytesIO import pandas as pd import numpy as np from rdkit.Chem import PandasTools from rdkit import Chem from rdkit.Chem import AllChem from rdkit.Chem import DataStructs from rdkit.Chem import rdMolDescriptors from rdkit.Chem import rdRGroupDecomposition from rdkit.Chem.Draw import IPythonConsole #Needed to show molecules from rdkit.Chem import Draw from rdkit.Chem import rdDepictor from rdkit.Chem.Draw import rdMolDraw2D from rdkit.Chem.Draw.MolDrawing import MolDrawing, DrawingOptions #Only needed if modifying defaults DrawingOptions.bondLineWidth=1.8 IPythonConsole.ipython_useSVG=True from rdkit import RDLogger RDLogger.DisableLog(&#39;rdApp.warning&#39;) import rdkit import py3Dmol print(rdkit.__version__) . . 2020.03.2 . Ring conformation using ETKDG v1 . Let&#39;s make a piperazine molecule for testing. I put atom map in my SMILES, so I can refer them easily later. . smiles = &#39;[NH:4]1[CH2:3][CH2:2][NH:1][CH2:6][CH2:5]1&#39; mol = Chem.MolFromSmiles(smiles) mol . N:4H C:3 C:2 HN:1 C:6 C:5 Let&#39;s use default parameter for embedding, which uses ETKDG version 1, and generate 500 conformers. . params = Chem.rdDistGeom.EmbedParameters() params.randomSeed = 0xf00d params.clearConfs = True mol_h = Chem.AddHs(mol) cids = AllChem.EmbedMultipleConfs(mol_h, numConfs=500, params=params) . atoms = {a.GetAtomMapNum(): a.GetIdx() for a in mol.GetAtoms() if a.GetAtomMapNum() &gt; 0} # torsion angle atom indices torsions = [ (atoms[1], atoms[2], atoms[3], atoms[4]), (atoms[2], atoms[3], atoms[4], atoms[5]), (atoms[3], atoms[4], atoms[5], atoms[6]), (atoms[4], atoms[5], atoms[6], atoms[1]), (atoms[5], atoms[6], atoms[1], atoms[2]), (atoms[6], atoms[1], atoms[2], atoms[3]), ] . I&#39;m going to define a simple check in chain vs boat vs twisted conformers of cyclohexane ring. If all six torsions are near +/- 60˚ (margin 30˚), it is considered a chair conformation. If two opposite torsions are close to 0˚ (margin 30˚) and the other remaining torsion angles are around +/- 60˚, then it is considered a boat conformation. The rest are considered as a twisted conformation. . def ring_conformers(mol): angles = [] confs = [] for conf in mol.GetConformers(): angs = np.abs([Chem.rdMolTransforms.GetDihedralDeg(conf, *t) for t in torsions]) angles.append(angs) if np.all(np.abs((angs - 60)/60) &lt; 0.66): confs.append(&#39;chair&#39;) elif angs[0] &lt; 30 and angs[3] &lt; 30 and np.all(np.abs((angs[[1,3,4,5]] - 60)/60) &lt; 0.5): confs.append(&#39;boat&#39;) elif angs[1] &lt; 30 and angs[4] &lt; 30 and np.all(np.abs((angs[[0,2,3,4]] - 60)/60) &lt; 0.5): confs.append(&#39;boat&#39;) elif angs[2] &lt; 30 and angs[5] &lt; 30 and np.all(np.abs((angs[[0,1,3,4]] - 60)/60) &lt; 0.5): confs.append(&#39;boat&#39;) else: confs.append(&#39;twisted&#39;) return confs . from collections import Counter confs = ring_conformers(mol_h) Counter(confs) . Counter({&#39;chair&#39;: 254, &#39;twisted&#39;: 228, &#39;boat&#39;: 18}) . So, I got 203 chair (40%), 18 boat (4%), and 279 twisted (56%) conformers. Let&#39;s take a look at some conformers and make sure we got the right conformers. First chair conformation. . from random import shuffle conf_idx = [i for i, c in enumerate(confs) if c == &#39;chair&#39;] shuffle(conf_idx) viewer = py3Dmol.view(width=300, height=300) viewer.addModel(Chem.MolToMolBlock(mol_h, confId=conf_idx[0]), &#39;mol&#39;) viewer.setStyle({&quot;stick&quot;:{}}) viewer.zoomTo() viewer.show() . . You appear to be running in JupyterLab (or JavaScript failed to load for some other reason). You need to install the 3dmol extension: jupyter labextension install jupyterlab_3dmol . And a boat conformation: . from random import shuffle conf_idx = [i for i, c in enumerate(confs) if c == &#39;boat&#39;] shuffle(conf_idx) viewer = py3Dmol.view(width=300, height=300) viewer.addModel(Chem.MolToMolBlock(mol_h, confId=conf_idx[0]), &#39;mol&#39;) viewer.setStyle({&quot;stick&quot;:{}}) viewer.zoomTo() viewer.show() . . You appear to be running in JupyterLab (or JavaScript failed to load for some other reason). You need to install the 3dmol extension: jupyter labextension install jupyterlab_3dmol . And finally a twisted conformation: . from random import shuffle conf_idx = [i for i, c in enumerate(confs) if c == &#39;twisted&#39;] shuffle(conf_idx) viewer = py3Dmol.view(width=300, height=300) viewer.addModel(Chem.MolToMolBlock(mol_h, confId=conf_idx[0]), &#39;mol&#39;) viewer.setStyle({&quot;stick&quot;:{}}) viewer.zoomTo() viewer.show() . . You appear to be running in JupyterLab (or JavaScript failed to load for some other reason). You need to install the 3dmol extension: jupyter labextension install jupyterlab_3dmol . Optimize Geometry using MMFF . Optimizing the Geometry using MMFF does the number of chair conformations and reduces twisted conformers. . for conf in mol_h.GetConformers(): AllChem.MMFFOptimizeMolecule(mol_h, confId=conf.GetId()) . confs = ring_conformers(mol_h) Counter(confs) . Counter({&#39;chair&#39;: 366, &#39;twisted&#39;: 131, &#39;boat&#39;: 3}) . Still about 50% of the conformers remain in the twisted conformation. . from random import shuffle conf_idx = [i for i, c in enumerate(confs) if c == &#39;twisted&#39;] shuffle(conf_idx) viewer = py3Dmol.view(width=300, height=300) viewer.addModel(Chem.MolToMolBlock(mol_h, confId=conf_idx[0]), &#39;mol&#39;) viewer.setStyle({&quot;stick&quot;:{}}) viewer.zoomTo() viewer.show() . . You appear to be running in JupyterLab (or JavaScript failed to load for some other reason). You need to install the 3dmol extension: jupyter labextension install jupyterlab_3dmol . ETKDG version 3 . I commented this issue on Greg&#39;s blog and was suggested to try ETKDG version 3 algorithm [ref]. Here&#39;s the result. . params = Chem.rdDistGeom.srETKDGv3() params.randomSeed = 0xf00d params.clearConfs = True cids = AllChem.EmbedMultipleConfs(mol_h, numConfs=500, params=params) . confs = ring_conformers(mol_h) Counter(confs) . Counter({&#39;chair&#39;: 500}) . All 500 chair conformation even without doing minimization! But strange things were happening when I actually looked at some conformers. It appears the torsion angle requirements are all satisfied, but the ring was actually very much twisted. . from random import shuffle #conf_idx = [i for i, c in enumerate(confs) if c == &#39;chair&#39;] #shuffle(conf_idx) conf_idx = [443] viewer = py3Dmol.view(width=300, height=300) viewer.addModel(Chem.MolToMolBlock(mol_h, confId=conf_idx[0]), &#39;mol&#39;) viewer.setStyle({&quot;stick&quot;:{}}) viewer.zoomTo() viewer.show() . . You appear to be running in JupyterLab (or JavaScript failed to load for some other reason). You need to install the 3dmol extension: jupyter labextension install jupyterlab_3dmol . conf_idx = 443 conf = mol_h.GetConformer(conf_idx) angs = [Chem.rdMolTransforms.GetDihedralDeg(conf, *t) for t in torsions] print(angs) . [-46.976379180669305, -46.49507991432351, 70.73380111873767, -47.28801834351138, -46.701253412582766, 70.21989187631925] . In the six-membered ring, the signs of the torsion angle should alternate around positive and negative 60 degree. In my naive torsion angle check, I only checked if the absolute value of the torsion angles were around 60˚. However, in this case the torsion angles are arranged in around (-60, -60, +60, -60, -60, +60), which makes it twisted. Let&#39;s count how many of these are in our conformers. . confs = [] sums = [] for conf in mol_h.GetConformers(): angs = np.array([Chem.rdMolTransforms.GetDihedralDeg(conf, *t) for t in torsions]) # check if pos/neg values are alternating signs = angs / np.abs(angs) if np.sum(signs[1:] + signs[:-1]) == 0: confs.append(&#39;chair&#39;) else: confs.append(&#39;twisted&#39;) . Counter(confs) . Counter({&#39;chair&#39;: 358, &#39;twisted&#39;: 142}) . OK, so turns out, 358 (72%) of the conformers were actually in chair conformation and 142 (28%) of the conformers were in twisted conformers. Let&#39;s take a look at some of the chair and twisted conformers. . from random import shuffle conf_idx = [i for i, c in enumerate(confs) if c == &#39;chair&#39;] shuffle(conf_idx) viewer = py3Dmol.view(width=300, height=300) viewer.addModel(Chem.MolToMolBlock(mol_h, confId=conf_idx[0]), &#39;mol&#39;) viewer.setStyle({&quot;stick&quot;:{}}) viewer.zoomTo() viewer.show() . . You appear to be running in JupyterLab (or JavaScript failed to load for some other reason). You need to install the 3dmol extension: jupyter labextension install jupyterlab_3dmol . A twisted conformer: . from random import shuffle conf_idx = [i for i, c in enumerate(confs) if c == &#39;twisted&#39;] shuffle(conf_idx) viewer = py3Dmol.view(width=300, height=300) viewer.addModel(Chem.MolToMolBlock(mol_h, confId=conf_idx[0]), &#39;mol&#39;) viewer.setStyle({&quot;stick&quot;:{}}) viewer.zoomTo() viewer.show() . . You appear to be running in JupyterLab (or JavaScript failed to load for some other reason). You need to install the 3dmol extension: jupyter labextension install jupyterlab_3dmol . Conclusion . I have used piperazine as an example because the molecule I was working had one, but I imagine cyclohexane may have the similar issue. The new ETKDG v3 algorithm does a good job getting chair conformation without FF optimization, but it could still produce seriously twisted conformers sometimes. User should check their conformer in such case with care before proceeding. . I have not looked into how RDKit&#39;s ETKDG algorithm is implemented yet. If it is simple, I might be able to contribute. Otherwise, I can use the conformation checker I used here to filter out non-chair conformers. .",
            "url": "https://sunhwan.github.io/blog/2021/02/24/RDKit-ETKDG-Piperazine.html",
            "relUrl": "/2021/02/24/RDKit-ETKDG-Piperazine.html",
            "date": " • Feb 24, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Learning Molecular Representation using Graph Neural Network - Molecular Graph",
            "content": "Motivation . I have used chemprop previously and got interested in how it works internally. I&#39;ve read their papers several times, but I&#39;m not a machine learning researcher, and how it handles the molecular reprentation using the graph neural network was not entirely clear to me. So, here I&#39;ll spend some time going through their code and try to understand it my own way. Most of the code was initially taken from the chemprop repository and I striped away the parts that I don&#39;t need for clarity. . %matplotlib inline import matplotlib.pyplot as plt import matplotlib from io import BytesIO import pandas as pd import numpy as np from IPython.display import SVG # RDKit import rdkit from rdkit.Chem import PandasTools from rdkit import Chem from rdkit.Chem import AllChem from rdkit.Chem import DataStructs from rdkit.Chem import rdMolDescriptors from rdkit.Chem import rdRGroupDecomposition from rdkit.Chem.Draw import IPythonConsole #Needed to show molecules from rdkit.Chem import Draw from rdkit.Chem import rdDepictor from rdkit.Chem.Draw import rdMolDraw2D from rdkit.Chem.Draw.MolDrawing import MolDrawing, DrawingOptions #Only needed if modifying defaults DrawingOptions.bondLineWidth=1.8 IPythonConsole.ipython_useSVG=True from rdkit import RDLogger RDLogger.DisableLog(&#39;rdApp.warning&#39;) print(rdkit.__version__) # pytorch import torch from torch.utils.data import DataLoader, Dataset, Sampler from torch import nn . . 2020.03.2 . Message passing neural network (MPNN) . Chemprop adopts a variant of graph neural network called &quot;directed message passing neural network (D-MPNN)&quot;. Let&#39;s first talk about MPNN and discuss the difference between the MPNN and D-MPNN later. . MPNN is a model that operates on an undirected graph, $G$ with a set of nodes $v$ and edges $e$. This is appealing because molecules can be thought as a graph with nodes (atoms) and edges (bonds). . . MPNN operates in two phases; molecular encoding phase and the feed-forward phase (the paper uses &quot;message passing phase&quot; and &quot;readout phase&quot;, respectively). In the molecular encoding phase, the features in the atoms and bonds are passed around T times to build a molecular representation of the molecule and the molecular properties are predicted in the feed-forward phase. The parameter T is also called &quot;depth&quot; and represents how &quot;far&quot; each nodes can &quot;see&quot;. . . Compared to a typical MPNN, the package chemprop adopts directed MPNN (D-MPNN) architecture using bond features. Although the molecular graph does not have a direction, one can treat each bond as two directed edges that goes opposite direction. One of the advantage of this approach is to prevent totters (message that goes back to itself because the first node is the its neighbor of neighbor). chemprop also uses bond feature, which is concatenated feature vector of atom and bond feature vectors. . Let&#39;s take a look at how chemprop featurizes atom and bond: . Atom Features . # we will define a class which holds various parameter for D-MPNN class TrainArgs: smiles_column = None no_cuda = False gpu = None num_workers = 8 batch_size = 50 atom_descriptors = None no_cache_mol = False dataset_type = &#39;regression&#39; task_names = [] seed = 0 atom_messages = False hidden_size = 300 bias = False depth = 3 dropout = 0.0 undirected = False aggregation = &#39;mean&#39; aggregation_norm = 100 @property def device(self) -&gt; torch.device: &quot;&quot;&quot;The :code:`torch.device` on which to load and process data and models.&quot;&quot;&quot; if not self.cuda: return torch.device(&#39;cpu&#39;) return torch.device(&#39;cuda&#39;, self.gpu) @device.setter def device(self, device: torch.device) -&gt; None: self.cuda = device.type == &#39;cuda&#39; self.gpu = device.index @property def cuda(self) -&gt; bool: &quot;&quot;&quot;Whether to use CUDA (i.e., GPUs) or not.&quot;&quot;&quot; return not self.no_cuda and torch.cuda.is_available() @cuda.setter def cuda(self, cuda: bool) -&gt; None: self.no_cuda = not cuda args = TrainArgs() . . For atom and bond features, we can take a look at the atom_features and bond_features function below. For example, the atom feature vector consists of one-hot encoding of atomic number, degree, formal charge, chirality, number of hydrogens, and hybridization. And the bond feature vector consists of one-hot encoding of bond type (single, double, triple, aromatic) and whether the bond is conjugated or not and whether in the ring or not. . MAX_ATOMIC_NUM = 100 ATOM_FEATURES = { &#39;atomic_num&#39;: list(range(MAX_ATOMIC_NUM)), &#39;degree&#39;: [0, 1, 2, 3, 4, 5], &#39;formal_charge&#39;: [-1, -2, 1, 2, 0], &#39;chiral_tag&#39;: [0, 1, 2, 3], &#39;num_Hs&#39;: [0, 1, 2, 3, 4], &#39;hybridization&#39;: [ Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2 ], } # Distance feature sizes PATH_DISTANCE_BINS = list(range(10)) THREE_D_DISTANCE_MAX = 20 THREE_D_DISTANCE_STEP = 1 THREE_D_DISTANCE_BINS = list(range(0, THREE_D_DISTANCE_MAX + 1, THREE_D_DISTANCE_STEP)) # len(choices) + 1 to include room for uncommon values; + 2 at end for IsAromatic and mass ATOM_FDIM = sum(len(choices) + 1 for choices in ATOM_FEATURES.values()) + 2 EXTRA_ATOM_FDIM = 0 BOND_FDIM = 14 def get_atom_fdim(): &quot;&quot;&quot;Gets the dimensionality of the atom feature vector.&quot;&quot;&quot; return ATOM_FDIM + EXTRA_ATOM_FDIM def get_bond_fdim(atom_messages=False): &quot;&quot;&quot;Gets the dimensionality of the bond feature vector. &quot;&quot;&quot; return BOND_FDIM + (not atom_messages) * get_atom_fdim() def onek_encoding_unk(value: int, choices: List[int]): encoding = [0] * (len(choices) + 1) index = choices.index(value) if value in choices else -1 encoding[index] = 1 return encoding def atom_features(atom: Chem.rdchem.Atom, functional_groups: List[int] = None): &quot;&quot;&quot;Builds a feature vector for an atom. &quot;&quot;&quot; features = onek_encoding_unk(atom.GetAtomicNum() - 1, ATOM_FEATURES[&#39;atomic_num&#39;]) + onek_encoding_unk(atom.GetTotalDegree(), ATOM_FEATURES[&#39;degree&#39;]) + onek_encoding_unk(atom.GetFormalCharge(), ATOM_FEATURES[&#39;formal_charge&#39;]) + onek_encoding_unk(int(atom.GetChiralTag()), ATOM_FEATURES[&#39;chiral_tag&#39;]) + onek_encoding_unk(int(atom.GetTotalNumHs()), ATOM_FEATURES[&#39;num_Hs&#39;]) + onek_encoding_unk(int(atom.GetHybridization()), ATOM_FEATURES[&#39;hybridization&#39;]) + [1 if atom.GetIsAromatic() else 0] + [atom.GetMass() * 0.01] # scaled to about the same range as other features if functional_groups is not None: features += functional_groups return features def bond_features(bond: Chem.rdchem.Bond): &quot;&quot;&quot;Builds a feature vector for a bond. &quot;&quot;&quot; if bond is None: fbond = [1] + [0] * (BOND_FDIM - 1) else: bt = bond.GetBondType() fbond = [ 0, # bond is not None bt == Chem.rdchem.BondType.SINGLE, bt == Chem.rdchem.BondType.DOUBLE, bt == Chem.rdchem.BondType.TRIPLE, bt == Chem.rdchem.BondType.AROMATIC, (bond.GetIsConjugated() if bt is not None else 0), (bond.IsInRing() if bt is not None else 0) ] fbond += onek_encoding_unk(int(bond.GetStereo()), list(range(6))) return fbond . Let&#39;s take a look at the example molecule and how atom and bond features actually look like: . smiles = &#39;c1ccccc1NC(=O)CC1cncc1&#39; mol = Chem.MolFromSmiles(smiles) mol . N H O N Below is the feature vector of the every atoms in the molecule. The first 100 elements represents the atomic number, followed by one hot encodings of degree (2), formal charge (0), chiral (false), total number of Hs (1), hybridization (SP2), aromaticity (1). Finally atomic mass (multiplied by 0.01) at the last entry. . feats = [] indices = [] for i in range(mol.GetNumAtoms()): atom = mol.GetAtomWithIdx(i) feat = atom_features(atom) feats.append(feat) indices.append(i) fig = plt.figure(figsize=(12, 0.25 * mol.GetNumAtoms())) ax = fig.add_subplot(111) im = ax.imshow(feats, interpolation=&#39;None&#39;, cmap=&#39;viridis&#39;, aspect=&#39;auto&#39;) plt.xlabel(&#39;atom feature&#39;) plt.ylabel(&#39;atom index&#39;) ax.set_yticks(indices) ax.set_yticklabels(indices) ax.tick_params(left=False) # remove the ticks plt.show() . . The atom index 0 and 7 are very similar since they are both carbon atoms and only slightly different in terms of aromaticity and the number of hydrogens attached. Let&#39;s take a look at the features of atom 0 and 7 side by side so we can see the difference more clearly. . import svgutils.compose as sc import svgutils.transform as sg from ipywidgets import interact, interactive, fixed from IPython.display import SVG from io import BytesIO atom1 = mol.GetAtomWithIdx(0) atom2 = mol.GetAtomWithIdx(7) feat1 = feats[0] feat2 = feats[7] # draw molecule with highlight d = rdMolDraw2D.MolDraw2DSVG(200, 150) rdMolDraw2D.PrepareAndDrawMolecule(d, mol, highlightAtoms=(atom1.GetIdx(), atom2.GetIdx())) d.FinishDrawing() mol_svg = d.GetDrawingText() # draw feature fig = plt.figure(figsize=(3, 0.91), dpi=150) ax = fig.add_subplot(111) im = ax.imshow([feat1, feat2], interpolation=&#39;nearest&#39;, cmap=&#39;viridis&#39;, aspect=&#39;auto&#39;) plt.xlabel(&#39;atom feature&#39;) ax.set_yticks([0, 1]) ax.set_yticklabels([atom1.GetIdx(), atom2.GetIdx()]) img = BytesIO() plt.tight_layout() plt.savefig(img, transparent=True, format=&#39;svg&#39;) plt.close(fig) feat_svg = img.getvalue().decode() # arrange figures fig1 = sg.fromstring(mol_svg) fig2 = sg.fromstring(feat_svg) plot1 = fig1.getroot() plot2 = fig2.getroot() plot1.moveto(10, -40) plot2.moveto(0, 65) svg = sc.Figure(&quot;16cm&quot;, &quot;6cm&quot;, plot1.scale(0.05), plot2.scale(0.05), ).tostr() SVG(svg) . . N H O N 2021-02-20T12:31:45.855792 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ # interactive plot does not work in the final page # import svgutils.compose as sc import svgutils.transform as sg from ipywidgets import interact, interactive, fixed from IPython.display import SVG from io import BytesIO def drawit(m, atomId=0): atom = m.GetAtomWithIdx(atomId) feat = atom_features(atom) # draw molecule with highlight d = rdMolDraw2D.MolDraw2DSVG(200, 150) rdMolDraw2D.PrepareAndDrawMolecule(d, m, highlightAtoms=(atom.GetIdx(),)) d.FinishDrawing() mol_svg = d.GetDrawingText() # draw feature fig = plt.figure(figsize=(3, 0.8), dpi=150) ax = fig.add_subplot(111) im = ax.imshow([feat], interpolation=&#39;nearest&#39;, cmap=&#39;viridis&#39;, aspect=&#39;auto&#39;) plt.xlabel(&#39;atom feature&#39;) ax.set_yticks([]) img = BytesIO() plt.tight_layout() plt.savefig(img, transparent=True, format=&#39;svg&#39;) plt.close(fig) feat_svg = img.getvalue().decode() # arrange figures fig1 = sg.fromstring(mol_svg) fig2 = sg.fromstring(feat_svg) plot1 = fig1.getroot() plot2 = fig2.getroot() plot1.moveto(10, -40) plot2.moveto(0, 65) svg = sc.Figure(&quot;16cm&quot;, &quot;6cm&quot;, plot1.scale(0.05), plot2.scale(0.05), ).tostr() return SVG(svg) interact(drawit, m=fixed(mol), atomId=(0, mol.GetNumAtoms()-1)); . . Bond Features . The bond feature is even more simpler. The bond feature vector consists of one-hot encoding of bond type (single, double, triple, aromatic) and whether the bond is conjugated or not and whether in the ring or not. . def bond_features(bond: Chem.rdchem.Bond): &quot;&quot;&quot;Builds a feature vector for a bond. &quot;&quot;&quot; if bond is None: fbond = [1] + [0] * (BOND_FDIM - 1) else: bt = bond.GetBondType() fbond = [ 0, # bond is not None bt == Chem.rdchem.BondType.SINGLE, bt == Chem.rdchem.BondType.DOUBLE, bt == Chem.rdchem.BondType.TRIPLE, bt == Chem.rdchem.BondType.AROMATIC, (bond.GetIsConjugated() if bt is not None else 0), (bond.IsInRing() if bt is not None else 0) ] fbond += onek_encoding_unk(int(bond.GetStereo()), list(range(6))) return fbond . # example molecule smiles = &#39;c1ccccc1NC(=O)CC1cncc1&#39; mol = Chem.MolFromSmiles(smiles) # let&#39;s take a look at the first bond bond1 = mol.GetBondWithIdx(0) # C=C aromatic bond assert bond1.GetBeginAtom().GetSymbol() == &#39;C&#39; assert bond1.GetEndAtom().GetSymbol() == &#39;C&#39; feat1 = bond_features(bond1) assert feat1[4] == 1 # aromatic assert feat1[6] == 1 # ring # highlight which bond with Idx 0 d = rdMolDraw2D.MolDraw2DSVG(300, 150) rdMolDraw2D.PrepareAndDrawMolecule(d, mol, highlightBonds=(0,)) d.FinishDrawing() svg = d.GetDrawingText() SVG(svg) . . N H O N fig = plt.figure(figsize=(4, 0.2)) ax = fig.add_subplot(111) im = ax.imshow(np.array(feat1)[np.newaxis, :], interpolation=&#39;None&#39;, cmap=&#39;viridis&#39;, aspect=&#39;auto&#39;) plt.xlabel(&#39;bond feature&#39;) ax.set(yticklabels=[]) ax.tick_params(left=False) # remove the ticks plt.show() . . Above is the feature vector of the 0th bond. This bond is aromatic, conjugated, and in a ring. Let&#39;s take a look at another bond feature and see how it is different from the 0th bond feature vector. . # highlight which bond with Idx 0 d = rdMolDraw2D.MolDraw2DSVG(300, 150) rdMolDraw2D.PrepareAndDrawMolecule(d, mol, highlightBonds=(7,)) d.FinishDrawing() svg = d.GetDrawingText() SVG(svg) . . N H O N bond2 = mol.GetBondWithIdx(7) # C=C aromatic bond feat2 = bond_features(bond2) fig = plt.figure(figsize=(4, 0.2)) ax = fig.add_subplot(111) im = ax.imshow(np.array(feat2)[np.newaxis, :], interpolation=&#39;None&#39;, cmap=&#39;viridis&#39;, aspect=&#39;auto&#39;) plt.xlabel(&#39;bond feature&#39;) ax.set(yticklabels=[]) ax.tick_params(left=False) # remove the ticks plt.show() . . Now you can see this bond is double bond, conjugated, and not in a ring. Let&#39;s display the bond feature vectors of every chemical bond in the molecule. . feats = [] indices = [] for i in range(mol.GetNumBonds()): bond = mol.GetBondWithIdx(i) feat = bond_features(bond) feats.append(feat) indices.append(i) fig = plt.figure(figsize=(4, 0.25 * mol.GetNumBonds())) ax = fig.add_subplot(111) im = ax.imshow(feats, interpolation=&#39;None&#39;, cmap=&#39;viridis&#39;, aspect=&#39;auto&#39;) plt.xlabel(&#39;bond feature&#39;) plt.ylabel(&#39;bond index&#39;) ax.set_yticks(indices) ax.set_yticklabels(indices) ax.tick_params(left=False) # remove the ticks plt.show() . . Graph featurizaation . chemprop defines the molecular graph as the code shown below. The MolGraph itself is pretty straightforward; iterates over atoms and bonds and stores atom feature and bond feature vectors into f_atoms and f_bonds attributes and construct neighboring atom indices. . class MolGraph: def __init__(self, mol, atom_descriptors=None): # Convert SMILES to RDKit molecule if necessary if type(mol) == str: mol = Chem.MolFromSmiles(mol) self.n_atoms = 0 # number of atoms self.n_bonds = 0 # number of bonds self.f_atoms = [] # mapping from atom index to atom features self.f_bonds = [] # mapping from bond index to concat(in_atom, bond) features self.a2b = [] # mapping from atom index to incoming bond indices self.b2a = [] # mapping from bond index to the index of the atom the bond is coming from self.b2revb = [] # mapping from bond index to the index of the reverse bond # Get atom features self.f_atoms = [atom_features(atom) for atom in mol.GetAtoms()] if atom_descriptors is not None: self.f_atoms = [f_atoms + descs.tolist() for f_atoms, descs in zip(self.f_atoms, atom_descriptors)] self.n_atoms = len(self.f_atoms) # Initialize atom to bond mapping for each atom for _ in range(self.n_atoms): self.a2b.append([]) # Get bond features for a1 in range(self.n_atoms): for a2 in range(a1 + 1, self.n_atoms): bond = mol.GetBondBetweenAtoms(a1, a2) if bond is None: continue f_bond = bond_features(bond) self.f_bonds.append(self.f_atoms[a1] + f_bond) self.f_bonds.append(self.f_atoms[a2] + f_bond) # Update index mappings b1 = self.n_bonds b2 = b1 + 1 self.a2b[a2].append(b1) # b1 = a1 --&gt; a2 self.b2a.append(a1) self.a2b[a1].append(b2) # b2 = a2 --&gt; a1 self.b2a.append(a2) self.b2revb.append(b2) self.b2revb.append(b1) self.n_bonds += 2 . Let&#39;s take a look at the atom and the bond features it builds internally. The atom features are exactly same as what we discussed in the previous section. . smiles = &#39;c1ccccc1NC(=O)CC1cncc1&#39; mol = Chem.MolFromSmiles(smiles) mol_graph = MolGraph(smiles) . # atom features fig = plt.figure(figsize=(12, 4)) ax = fig.add_subplot(111) im = ax.imshow(mol_graph.f_atoms, interpolation=&#39;None&#39;, cmap=&#39;viridis&#39;, aspect=&#39;auto&#39;) ax.set_yticks(list(range(mol_graph.n_atoms))) ax.set_yticklabels(list(range(mol_graph.n_atoms))) ax.tick_params(left=False) # remove the ticks plt.xlabel(&#39;atom feature&#39;) plt.ylabel(&#39;atom index&#39;) plt.show() . . The &quot;bond&quot; in the molecular graph represents directed bonds. For example, there are two bonds, b1 and b2 between the atoms a1 and a2. The bond b1 is a bond from the atom a1 to atom a2 and the bond b2 is a bond from the atom a2 to a1. The bond feature is then constructed by concatenate the incoming atom (originating atom) feature and the bond feature. . # bond features : atom feature + bond feature # bond features are added as nested atoms loop. # For each bond, a1-&gt;a2 and a2-&gt;a1 are added. So, more bond features than NumBonds fig = plt.figure(figsize=(12, 8)) ax = fig.add_subplot(111) im = ax.imshow(mol_graph.f_bonds, interpolation=&#39;None&#39;, cmap=&#39;viridis&#39;, aspect=&#39;auto&#39;) ax.set_yticks(list(range(mol_graph.n_bonds))) ax.set_yticklabels(list(range(mol_graph.n_bonds))) ax.tick_params(left=False) # remove the ticks plt.xlabel(&#39;bond feature&#39;) plt.ylabel(&#39;bond index&#39;) plt.show() . . The attributes a2b, b2a, and b2revb contains various mapping of atom index to bond indices, bond index to atom index, and reverse bond index. These are required for the message passing to work properly. . Message passing . Now we are ready to dig into the most interesting part of MPNN architecture. The messages are passed around according to the connectivity, and the message evolves as it travels around the nodes. . The message passing phase consists of $T$ steps of update cycles. In each step $t$, hidden state hidden state $h_{vw}^t$ and message $m_{vw}^t$ are updated using message function $M_t$ and vertex update function $U_t$. Each message and hidden states are associated with nodes $v$ and $w$. Note the direction of message matters, so $h_{vw}^t$ and $m_{vw}^t$ are different from $h_{wv}^t$ and $m_{wv}^t$. . $$m_v^{t+1} = large sum_{k in {N(v)w }}^{} M_t(x_v, x_w, h_{kv}^t)$$ . $$h_{vw}^{t+1} = U_t(h_{vw}^t, m_{vw}^{t+1})$$ . The initial hidden state for each node is defined as . $$h_{vw}^0 = tau (W_i mathrm{cat} (x_v, e_{vw}))$$ . where $W_i$ is a learned matrix ($ mathbb{R}^{h times h_i}$), $ mathrm{cat} (x_v, e_{vw})$ is the concatenation of atom features ($ mathbb{R}^{h_i}$), $x_v$ and the bond feature $e_{vw}$ for bond $vw$, and the $ tau$ is the activation function. . chemprop uses very simple message passing function and edge update function: . $$M_t(x_v, x_w, h_{vw}^t) = h_{vw}^t$$ . $$U_t(h_{vw}^t, m_{vw}^{t+1}) = U(h_{vw}^t, m_{vw}^{t+1}) = tau (h_{vw}^0 + W_m m_{vw}^{t+1})$$. . The $W_m$ is a learned matrix ($ mathbb{R}^{h times h}$) . Finally, the atom representation of molecule is computed by summing over all incoming bond features. . $$m_v = sum_{k in N(v)} h_{kv}^t$$ . $$h_v = tau(W_a mathrm{cat} (x_v, m_v))$$ . where $W_a$ is a learned matrix ($ mathbb{R}^{h times h}$). The readout phase of the D-MPNN uses the readout function, $R$, which is a simple summation of all the atom hidden states, which subsequently used in a feed-forward network for predicting the molecular properties. . $$h = sum_{v in G} h_v$$ . Let&#39;s get into to the code and see how above is implemented. . Initial message . The initial hidden state for each node is defined as . $$h_{vw}^0 = tau (W_i mathrm{cat} (x_v, e_{vw}))$$ . # prepare the tensors for message passing bond_fdim = get_bond_fdim() atom_fdim = get_atom_fdim() n_atoms = 1 # number of atoms (start at 1 b/c need index 0 as padding) n_bonds = 1 # number of bonds (start at 1 b/c need index 0 as padding) a_scope = [] # list of tuples indicating (start_atom_index, num_atoms) for each molecule b_scope = [] # list of tuples indicating (start_bond_index, num_bonds) for each molecule # All start with zero padding so that indexing with zero padding returns zeros f_atoms = [[0] * atom_fdim] # atom features f_bonds = [[0] * bond_fdim] # combined atom/bond features a2b = [[]] # mapping from atom index to incoming bond indices b2a = [0] # mapping from bond index to the index of the atom the bond is coming from b2revb = [0] # mapping from bond index to the index of the reverse bond f_atoms.extend(mol_graph.f_atoms) f_bonds.extend(mol_graph.f_bonds) for a in range(mol_graph.n_atoms): a2b.append([b + n_bonds for b in mol_graph.a2b[a]]) for b in range(mol_graph.n_bonds): b2a.append(n_atoms + mol_graph.b2a[b]) b2revb.append(n_bonds + mol_graph.b2revb[b]) a_scope.append((n_atoms, mol_graph.n_atoms)) b_scope.append((n_bonds, mol_graph.n_bonds)) n_atoms += mol_graph.n_atoms n_bonds += mol_graph.n_bonds max_num_bonds = max(1, max(len(in_bonds) for in_bonds in a2b)) # max with 1 to fix a crash in rare case of all single-heavy-atom mols f_atoms = torch.FloatTensor(f_atoms) f_bonds = torch.FloatTensor(f_bonds) a2b = torch.LongTensor([a2b[a] + [0] * (max_num_bonds - len(a2b[a])) for a in range(n_atoms)]) b2a = torch.LongTensor(b2a) b2revb = torch.LongTensor(b2revb) . . # define and initialize leanred matrix input_dim = get_bond_fdim() atom_fdim = get_atom_fdim() W_i = nn.Linear(input_dim, args.hidden_size, bias=args.bias) w_h_input_size = args.hidden_size W_h = nn.Linear(w_h_input_size, args.hidden_size, bias=args.bias) W_o = nn.Linear(atom_fdim + args.hidden_size, args.hidden_size) act_func = nn.ReLU() # initial message input = W_i(torch.FloatTensor(f_bonds)) # num_bonds x hidden_size message = act_func(input) . fig = plt.figure(figsize=(12, 14)) ax = fig.add_subplot(211) im = ax.imshow(mol_graph.f_bonds, interpolation=&#39;None&#39;, cmap=&#39;viridis&#39;, aspect=&#39;auto&#39;) ax.set_yticks(list(range(mol_graph.n_bonds))) ax.set_yticklabels(list(range(mol_graph.n_bonds))) ax.tick_params(left=False) # remove the ticks plt.xlabel(&#39;bond feature&#39;) plt.ylabel(&#39;bond index&#39;) plt.title(&#39;Initial bond feature&#39;) ax = fig.add_subplot(212) im = ax.imshow(message.detach().numpy(), interpolation=&#39;None&#39;, cmap=&#39;viridis&#39;, aspect=&#39;auto&#39;) ax.set_yticks(list(range(mol_graph.n_bonds + 1))) ax.set_yticklabels(list(range(mol_graph.n_bonds + 1))) ax.tick_params(left=False) # remove the ticks plt.xlabel(&#39;hidden state&#39;) plt.ylabel(&#39;bond index&#39;) plt.title(&#39;Initial message&#39;) plt.show() . . Message Passing . The message passing phase consists of $T$ steps of composing messages and hidden state according to the message function and the update function. . $$m_{vw}^{t+1} = large sum_{k in {N(v)w }}^{} h_{kv}^t$$ . $$h_{vw}^{t+1} = tau (h_{vw}^0 + W_m m_{vw}^{t+1})$$ . Below, we will try 3 cycles of message passing. . def index_select_ND(source: torch.Tensor, index: torch.Tensor) -&gt; torch.Tensor: &quot;&quot;&quot;Selects the message features from source corresponding to the atom or bond indices in index. &quot;&quot;&quot; index_size = index.size() # (num_atoms/num_bonds, max_num_bonds) suffix_dim = source.size()[1:] # (hidden_size,) final_size = index_size + suffix_dim # (num_atoms/num_bonds, max_num_bonds, hidden_size) target = source.index_select(dim=0, index=index.view(-1)) # (num_atoms/num_bonds * max_num_bonds, hidden_size) target = target.view(final_size) # (num_atoms/num_bonds, max_num_bonds, hidden_size) return target for depth in range(3): # m(a1 -&gt; a2) = [sum_{a0 in nei(a1)} m(a0 -&gt; a1)] - m(a2 -&gt; a1) # message a_message = sum(nei_a_message) rev_message nei_a_message = index_select_ND(message, a2b) # num_atoms x max_num_bonds x hidden a_message = nei_a_message.sum(dim=1) # num_atoms x hidden rev_message = message[b2revb] # num_bonds x hidden message = a_message[b2a] - rev_message # num_bonds x hidden message = W_h(message) message = act_func(input + message) # num_bonds x hidden_size fig = plt.figure(figsize=(12, 7)) ax = fig.add_subplot(111) im = ax.imshow(a_message[b2a].detach().numpy(), interpolation=&#39;None&#39;, cmap=&#39;viridis&#39;, aspect=&#39;auto&#39;) ax.set_yticks(list(range(mol_graph.n_bonds + 1))) ax.set_yticklabels(list(range(mol_graph.n_bonds + 1))) ax.tick_params(left=False) # remove the ticks plt.xlabel(&#39;hidden state&#39;) plt.ylabel(&#39;bond index&#39;) plt.title(&#39;Messages after 3 steps of message passing&#39;) plt.show() . . Readout Phase . Finally, the atom representation of molecule is computed by summing over for all incoming bond features. . $$m_v = sum_{k in N(v)} h_{kv}^t$$ . $$h_v = tau(W_a mathrm{cat} (x_v, m_v))$$ . where $W_a$ is a learned matrix ($ mathbb{R}^{h times h}$). The readout phase of the D-MPNN uses the readout function, $R$, which is a simple summation of all the atom hidden states, which subsequently used in a feed-forward network for predicting the molecular properties. . $$h = sum_{v in G} h_v$$ . nei_a_message = index_select_ND(message, a2b) # num_atoms x max_num_bonds x hidden a_message = nei_a_message.sum(dim=1) # num_atoms x hidden a_input = torch.cat([f_atoms, a_message], dim=1) # num_atoms x (atom_fdim + hidden) atom_hiddens = act_func(W_o(a_input)) # num_atoms x hidden . fig = plt.figure(figsize=(12, 4)) ax = fig.add_subplot(111) im = ax.imshow(atom_hiddens.detach().numpy(), interpolation=&#39;None&#39;, cmap=&#39;viridis&#39;, aspect=&#39;auto&#39;) ax.set_yticks(list(range(mol_graph.n_atoms + 1))) ax.set_yticklabels(list(range(mol_graph.n_atoms + 1))) ax.tick_params(left=False) # remove the ticks plt.xlabel(&#39;hidden state&#39;) plt.ylabel(&#39;atom index&#39;) plt.title(&#39;Final hidden state for each atom&#39;) plt.show() . . Now we sum the hidden states to form the final molecular vector. This vector is called &quot;learned molecular vector&quot; and used in property prediction using feed-forward network. At this point, we have not trained the leanred matrices and the hidden states are close to random numbers. In the next post, I&#39;ll try to explore how these hidden states and the leanred molecular vector evolves as we train the neural network. . This learned molecular vector is equivalent to molecular fingerprint, however, unlike molecular fingerprint, this representation can change for different dataset to better represents the nature of the data, which is the basis of how graph neural network can outperform the traditional machine learning approaches using fingerprint only. . mol_vecs = [] for i, (a_start, a_size) in enumerate(a_scope): if a_size == 0: mol_vecs.append(cached_zero_vector) else: cur_hiddens = atom_hiddens.narrow(0, a_start, a_size) mol_vec = cur_hiddens # (num_atoms, hidden_size) mol_vec = mol_vec.sum(dim=0) / a_size mol_vecs.append(mol_vec) mol_vecs = torch.stack(mol_vecs, dim=0) # (num_molecules, hidden_size) . fig = plt.figure(figsize=(12, 0.2)) ax = fig.add_subplot(111) im = ax.imshow(mol_vecs.detach().numpy(), interpolation=&#39;None&#39;, cmap=&#39;viridis&#39;, aspect=&#39;auto&#39;) ax.set_yticks([]) ax.tick_params(left=False) # remove the ticks plt.xlabel(&#39;hidden state&#39;) plt.title(&#39;Learned Molecular Vector&#39;) plt.show() . . Conclusion . Graph neural network fits well in representing molecule. It was interesting to take a look into how chemprop compute the learned molecular vector. This gave me a better understanding of MPNN and some aspects that I could experiment with. . Right before the readout phase, the atom-centered message or hidden state associated for the edge, could be used for atom centered properties, such as pKa or NMR chemical shift. | The atom and bond feature appears very simple. chemprop has an option that can use features from other toolkit and it does improves the performance of prediction. | The network only considered bonded interactions, however, atoms do interact even if they are not bonded. Such interaction is completely ignored in MPNN. | The rate of information transfer can be faster if we adopt a coarse network where the node are connected to not only neighbors but neighbor-of-neighbors or a network of functional groups. | Some kind of attention algorithm might also be useful to improve interpretability of the network. | . In the next post, I&#39;ll train a GCNN and examine how the learned mlecular vector evolves after a training. .",
            "url": "https://sunhwan.github.io/blog/2021/02/20/Learning-Molecular-Representation-Using-Graph-Neural-Network-Molecular-Graph.html",
            "relUrl": "/2021/02/20/Learning-Molecular-Representation-Using-Graph-Neural-Network-Molecular-Graph.html",
            "date": " • Feb 20, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Ligand SASA in Protein Pocket",
            "content": "Motivation . Solvent-accessible surface area (SASA) is an important descriptor in ligand binding. The extent of ligand SASA value decrease upon binding indicates whether the ligand is deeply buried or not upon binding to the pocket. RDKit provides SASA value calculation, which is based on FreeSASA package. . %matplotlib inline import matplotlib.pyplot as plt from io import BytesIO import pandas as pd import numpy as np from rdkit.Chem import PandasTools from rdkit import Chem from rdkit.Chem import AllChem from rdkit.Chem import DataStructs from rdkit.Chem import rdMolDescriptors from rdkit.Chem import rdRGroupDecomposition from rdkit.Chem.Draw import IPythonConsole #Needed to show molecules from rdkit.Chem import Draw from rdkit.Chem import rdDepictor from rdkit.Chem.Draw import rdMolDraw2D from rdkit.Chem.Draw.MolDrawing import MolDrawing, DrawingOptions #Only needed if modifying defaults DrawingOptions.bondLineWidth=1.8 IPythonConsole.ipython_useSVG=True from rdkit import RDLogger RDLogger.DisableLog(&#39;rdApp.warning&#39;) import rdkit import py3Dmol print(rdkit.__version__) . . 2020.03.2 . Example . Let&#39;s use ABL2-Imatinib complex (PDB:3GVU) for an example. I first downloaded the file and removed the water and the bound ligands. . prot = Chem.MolFromPDBFile(&#39;files/3gvu.pdb&#39;) lig = Chem.MolFromMolFile(&#39;files/STI.sdf&#39;) viewer = py3Dmol.view(width=300, height=300) viewer.addModel(Chem.MolToPDBBlock(prot), &#39;mol&#39;) viewer.addModel(Chem.MolToPDBBlock(lig), &#39;pdb&#39;) viewer.setStyle({&#39;chain&#39;: &#39;A&#39;}, {&#39;cartoon&#39;:{&#39;color&#39;:&#39;spectrum&#39;}}) viewer.setStyle({&#39;resn&#39;: &#39;UNL&#39;}, {&#39;stick&#39;:{}}) viewer.zoomTo({&#39;resn&#39;: &#39;UNL&#39;}) viewer.show() . You appear to be running in JupyterLab (or JavaScript failed to load for some other reason). You need to install the 3dmol extension: jupyter labextension install jupyterlab_3dmol . Let&#39;s define a function that compute SASA. The code is taken from here . from rdkit.Chem import rdFreeSASA # compute ligand SASA lig_h = Chem.AddHs(lig, addCoords=True) # Get Van der Waals radii (angstrom) ptable = Chem.GetPeriodicTable() radii = [ptable.GetRvdw(atom.GetAtomicNum()) for atom in lig_h.GetAtoms()] # Compute solvent accessible surface area lig_sasa = rdFreeSASA.CalcSASA(lig_h, radii) . comp = Chem.CombineMols(prot, lig) comp_h = Chem.AddHs(comp, addCoords=True) # Get Van der Waals radii (angstrom) ptable = Chem.GetPeriodicTable() radii = [ptable.GetRvdw(atom.GetAtomicNum()) for atom in comp_h.GetAtoms()] # Compute solvent accessible surface area comp_sasa = rdFreeSASA.CalcSASA(comp_h, radii) . print(lig_sasa, comp_sasa) . 849.5659988403745 14213.44708409188 . Note that comp_sasa is the overall SASA of both protein and ligand. We want to compute the SASA of ligand only while in the binding pocket. RDKit stores the per-atom SASA values in the atom object. . comp_lig = Chem.GetMolFrags(comp_h, asMols=True)[-1] # ligand is the last component lig_sasa_free = 0 for a in lig_h.GetAtoms(): lig_sasa_free += float(a.GetProp(&quot;SASA&quot;)) lig_sasa_bound = 0 for a in comp_lig.GetAtoms(): lig_sasa_bound += float(a.GetProp(&quot;SASA&quot;)) . print(&quot;Ligand SASA (free) =&quot;, lig_sasa_free) print(&quot;Ligand SASA (bound) =&quot;, lig_sasa_bound) print(&quot;Ligand SASA difference =&quot;, lig_sasa_free - lig_sasa_bound) . Ligand SASA (free) = 849.5659988403745 Ligand SASA (bound) = 68.71464272335984 Ligand SASA difference = 780.8513561170147 . Reduce computation time . Let&#39;s put above in a functional call and measure the timing. . from rdkit.Chem import rdFreeSASA def compute_sasa(mol): # Get Van der Waals radii (angstrom) ptable = Chem.GetPeriodicTable() radii = [ptable.GetRvdw(atom.GetAtomicNum()) for atom in mol.GetAtoms()] # Compute solvent accessible surface area sasa = rdFreeSASA.CalcSASA(mol, radii) return sasa def compute_ligand_sasa_pocket(prot, lig): # compute complex SASA comp = Chem.CombineMols(prot, lig) comp_h = Chem.AddHs(comp, addCoords=True) comp_sasa = compute_sasa(comp_h) # compute ligand SASA in pocket comp_lig = Chem.GetMolFrags(comp_h, asMols=True, sanitizeFrags=False)[-1] # ligand is the last component lig_sasa_bound = sum([float(a.GetProp(&quot;SASA&quot;)) for a in comp_lig.GetAtoms()]) return lig_sasa_bound . sasa_free = compute_sasa(lig) sasa_bound = compute_ligand_sasa_pocket(prot, lig) print(&quot;Ligand SASA (free) =&quot;, sasa_free) print(&quot;Ligand SASA (bound) =&quot;, sasa_bound) print(&quot;Ligand SASA difference =&quot;, sasa_free - sasa_bound) . Ligand SASA (free) = 767.8208065148369 Ligand SASA (bound) = 68.71464272335984 Ligand SASA difference = 699.1061637914771 . Let&#39;s measure the timing of this function call. I suspect this is not particularly fast because the code needs to compute SASA of all protein atoms as well. Perhaps we can make this faster by only computing SASA within certain distance from the ligand atoms. . %timeit compute_ligand_sasa_pocket(prot, lig) . 323 ms ± 1.39 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . def compute_ligand_sasa_pocket_cutoff(prot, lig, cutoff=8): lig_conf = lig.GetConformer() lig_xyz = lig_conf.GetPositions() prot_conf = prot.GetConformer() prot_xyz = conf.GetPositions() # minimum distance between protein atoms and ligand atoms r = np.min(np.linalg.norm(prot_xyz[:, np.newaxis, :] - lig_xyz[np.newaxis, :, :], axis=2), axis=1) indices = np.argwhere(r &gt; cutoff).flatten() mol = Chem.RWMol(prot) for idx in sorted(indices, reverse=True): mol.RemoveAtom(int(idx)) return compute_ligand_sasa_pocket(mol, lig) . sasa_free = compute_sasa(lig) sasa_bound = compute_ligand_sasa_pocket_cutoff(prot, lig, 5) print(&quot;Ligand SASA (free) =&quot;, sasa_free) print(&quot;Ligand SASA (bound) =&quot;, sasa_bound) print(&quot;Ligand SASA difference =&quot;, sasa_free - sasa_bound) . Ligand SASA (free) = 767.8208065148369 Ligand SASA (bound) = 89.35880016654482 Ligand SASA difference = 678.4620063482921 . With the cutoff distance of 5 Å, we get slightly larger SASA value in the bound state. We can improve this by increasing the cutoff distance. . cutoff_values = [5, 6, 7, 8, 9, 10] sasa_bound_values = [compute_ligand_sasa_pocket_cutoff(prot, lig, c) for c in cutoff_values] plt.plot(cutoff_values, sasa_bound_values) plt.xlabel(&#39;Cutoff&#39;) plt.ylabel(&#39;SASA&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-02-05T11:44:10.373090 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ The SASA value converged after 7 Å cutoff. Let&#39;s compute the function call speed using 8 Å as cutoff. . %timeit compute_ligand_sasa_pocket_cutoff(prot, lig, 8) . 229 ms ± 20.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . In my laptop, I got 323 ms (without cutoff) vs 229 ms (with cutoff), which represents about 40% speedup. .",
            "url": "https://sunhwan.github.io/blog/2021/02/04/RDKit-Protein-Ligand-SASA.html",
            "relUrl": "/2021/02/04/RDKit-Protein-Ligand-SASA.html",
            "date": " • Feb 4, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Build 3D coordinates of congeneric series",
            "content": "Motivation . I often have to modify a given molecule to introduce a set of modification to make congeneric series. AllChem.ConstrainedEmbed in RDKit could provide such function. See below blog posts for an example: . http://rdkit.blogspot.com/2013/12/using-allchemconstrainedembed.html | https://iwatobipen.wordpress.com/2019/06/04/constrain-embedding-with-mcs-as-a-core-rdkit-chemoinformatics/ | https://www.blopig.com/blog/2019/06/constrained-embedding-with-rdkit/ | . However, AllChem.ConstrinedEmbed uses MCS algorithm, and the MCS sometimes did not yielded the core that I desired, which resulted in a completely wrong alignment between the parent and the newly modified molecule. . So, I wrote a small function that takes the SMARTS pattern, and explicitly takes the coordinates then embed the rest of the coordinates. . from io import BytesIO import pandas as pd import numpy as np from rdkit.Chem import PandasTools from rdkit import Chem from rdkit.Chem import AllChem from rdkit.Chem import DataStructs from rdkit.Chem import rdMolDescriptors from rdkit.Chem import rdRGroupDecomposition from rdkit.Chem.Draw import IPythonConsole #Needed to show molecules from rdkit.Chem import Draw from rdkit.Chem import rdDepictor from rdkit.Chem.Draw import rdMolDraw2D from rdkit.Chem.Draw.MolDrawing import MolDrawing, DrawingOptions #Only needed if modifying defaults DrawingOptions.bondLineWidth=1.8 IPythonConsole.ipython_useSVG=True from rdkit import RDLogger RDLogger.DisableLog(&#39;rdApp.warning&#39;) import rdkit import py3Dmol print(rdkit.__version__) . . 2020.03.2 . Example . Let&#39;s use Imatinib for the parent molecule as an example. . imatinib = Chem.MolFromMolFile(&#39;files/STI.sdf&#39;) imatinib_2d = Chem.RemoveHs(imatinib) AllChem.Compute2DCoords(imatinib_2d) imatinib_2d . N N N NH HN O N N And suppose we want to replace the piperazine to a phenyl. . mol_new = Chem.MolFromSmiles(&#39;Cc1ccc(NC(=O)c2ccc(c3ccccc3)cc2)cc1Nc1nccc(-c2cccnc2)n1&#39;) mol_new . HN O NH N N N SMARTS Pattern . smarts = &#39;[#6]1:[#6]:[#7]:[#6]:[#6](:[#6]:1)-[#6]1:[#7]:[#6](:[#7]:[#6]:[#6]:1)-[#7]-[#6]1:[#6]:[#6](:[#6]:[#6]:[#6]:1-[#6])-[#7]-[#6](-[#6]1:[#6]:[#6]:[#6](:[#6]:[#6]:1))=[#8]&#39; p = Chem.MolFromSmarts(smarts) p . N N N N N O imatinib_2d.GetSubstructMatch(p) imatinib_2d . N N N NH HN O N N mol_new.GetSubstructMatch(p) mol_new . HN O NH N N N Build the new conformer . from rdkit.Chem import rdFMCS from rdkit.Chem import rdDistGeom from rdkit.Chem.rdForceFieldHelpers import UFFGetMoleculeForceField def build(ref, m, smarts): mol = Chem.AddHs(m) p = Chem.MolFromSmarts(smarts) match1 = ref.GetSubstructMatch(p) match2 = mol.GetSubstructMatch(p) coordMap = {} coreConf = ref.GetConformer(0) for i, idxI in enumerate(match2): corePtI = coreConf.GetAtomPosition(match1[i]) coordMap[idxI] = corePtI algMap = [(j, match1[i]) for i, j in enumerate(match2)] tdist = 0.25 confid = AllChem.EmbedMolecule(mol, coordMap=coordMap) rms = AllChem.AlignMol(mol, ref, atomMap=algMap) ff = UFFGetMoleculeForceField(mol, confId=0) conf = ref.GetConformer() for i in range(len(match1)): p = conf.GetAtomPosition(match1[i]) pIdx = ff.AddExtraPoint(p.x, p.y, p.z, fixed=True) - 1 ff.AddDistanceConstraint(pIdx, match2[i], 0, tdist, 100.) ff.Initialize() n = 4 more = ff.Minimize(energyTol=1e-4, forceTol=1e-3) while more and n: more = ff.Minimize(energyTol=1e-4, forceTol=1e-3) n -= 1 # realign rms = AllChem.AlignMol(mol, ref, atomMap=algMap) return mol . mol_new = build(imatinib, mol_new, smarts) . You can see the new molecule have very similar coordinates except the newly added phenyl ring in 3D. . viewer = py3Dmol.view(width=300, height=300) viewer.addModel(Chem.MolToMolBlock(imatinib), &#39;mol&#39;) viewer.addModel(Chem.MolToMolBlock(mol_new), &#39;mol&#39;) viewer.setStyle({&quot;stick&quot;:{}}) viewer.zoomTo() viewer.show() . You appear to be running in JupyterLab (or JavaScript failed to load for some other reason). You need to install the 3dmol extension: jupyter labextension install jupyterlab_3dmol . Note you can perform a fragment grafting instead of embedding. See https://pschmidtke.github.io/blog/rdkit/3d-editor/2021/01/23/grafting-fragments.html .",
            "url": "https://sunhwan.github.io/blog/2021/01/29/RDKit-Embed-Congeneric-Series.html",
            "relUrl": "/2021/01/29/RDKit-Embed-Congeneric-Series.html",
            "date": " • Jan 29, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://sunhwan.github.io/blog/2020/02/20/test.html",
            "relUrl": "/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sunhwan.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sunhwan.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}